{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c649a4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 16:30:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/11 16:30:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/11 16:30:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"24121003_yellowtaxi_trip_count\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60757aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "trip_files = '/trips/*'\n",
    "zone_file = 'taxi+_zone_lookup.csv'\n",
    "directory = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d7049be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_df = spark.read.csv(f'file:///{directory}/{trip_files}', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c778fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_df = spark.read.csv(f'file:///{directory}/{zone_file}', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "199bb554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c938953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74eeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView('trips')\n",
    "zone_df.createOrReplaceTempView('zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bc17244",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select\n",
    "t.VendorID,\n",
    "TO_DATE(t.tpep_pickup_datetime) as pickup_date,\n",
    "TO_DATE(t.tpep_dropoff_datetime) as dropoff_date,\n",
    "HOUR(t.tpep_pickup_datetime)  as pickup_time,\n",
    "HOUR(t.tpep_dropoff_datetime) as dropoff_time,\n",
    "t.passenger_count,\n",
    "t.trip_distance,\n",
    "t.tip_amount,\n",
    "t.total_amount,\n",
    "t.payment_type,\n",
    "pz.Zone as pickup_zone,\n",
    "dz.Zone as dropoff_zone\n",
    "from trips t\n",
    "LEFT JOIN zone pz ON t.PULocationID = pz.LocationID\n",
    "LEFT JOIN zone dz ON t.DOLocationID = dz.LocationID\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edb299c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fce33dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15000700"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff02e171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+-----------+------------+---------------+-------------+----------+------------+------------+-----------------+--------------+\n",
      "|VendorID|pickup_date|dropoff_date|pickup_time|dropoff_time|passenger_count|trip_distance|tip_amount|total_amount|payment_type|      pickup_zone|  dropoff_zone|\n",
      "+--------+-----------+------------+-----------+------------+---------------+-------------+----------+------------+------------+-----------------+--------------+\n",
      "|       2| 2021-03-01|  2021-03-01|          0|           0|              1|          0.0|       0.0|         4.3|           2|               NV|            NV|\n",
      "|       2| 2021-03-01|  2021-03-01|          0|           0|              1|          0.0|       0.0|         3.8|           2|   Manhattanville|Manhattanville|\n",
      "|       2| 2021-03-01|  2021-03-01|          0|           0|              1|          0.0|       0.0|         4.8|           2|   Manhattanville|Manhattanville|\n",
      "|       1| 2021-03-01|  2021-03-01|          0|           0|              0|         16.5|     11.65|       70.07|           1|LaGuardia Airport|            NA|\n",
      "|       2| 2021-03-01|  2021-03-01|          0|           0|              1|         1.13|      1.86|       11.16|           1|     East Chelsea|            NV|\n",
      "+--------+-----------+------------+-----------+------------+---------------+-------------+----------+------------+------------+-----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comb_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eea87b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df.createOrReplaceTempView('comb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "930aa3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select pickup_date, pickup_time \n",
    "from comb \n",
    "where pickup_time>0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae69f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|pickup_date|pickup_time|\n",
      "+-----------+-----------+\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-03-01|         22|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d3f4256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|pickup_date|pickup_time|\n",
      "+-----------+-----------+\n",
      "| 2009-01-01|          0|\n",
      "| 2008-12-31|         23|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          1|\n",
      "| 2009-01-01|          0|\n",
      "| 2008-12-31|         23|\n",
      "| 2008-12-31|         23|\n",
      "| 2008-12-31|         23|\n",
      "| 2008-12-31|         23|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|         16|\n",
      "| 2009-01-01|         16|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select pickup_date, pickup_time \n",
    "from comb \n",
    "where pickup_date<'2020-12-31'\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3910fd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, VendorID: string, pickup_time: string, dropoff_time: string, passenger_count: string, trip_distance: string, tip_amount: string, total_amount: string, payment_type: string, pickup_zone: string, dropoff_zone: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e4755a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [cast(tpep_pickup_datetime#41 as date) AS pickup_date#100, hour(cast(tpep_pickup_datetime#41 as timestamp), Some(Asia/Seoul)) AS pickup_time#102]\n",
      "+- *(3) BroadcastHashJoin [DOLocationID#48], [LocationID#106], LeftOuter, BuildRight, false\n",
      "   :- *(3) Project [tpep_pickup_datetime#41, DOLocationID#48]\n",
      "   :  +- *(3) BroadcastHashJoin [PULocationID#47], [LocationID#92], LeftOuter, BuildRight, false\n",
      "   :     :- *(3) Filter (isnotnull(tpep_pickup_datetime#41) AND (cast(tpep_pickup_datetime#41 as date) < 18627))\n",
      "   :     :  +- FileScan csv [tpep_pickup_datetime#41,PULocationID#47,DOLocationID#48] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#41), (cast(tpep_pickup_datetime#41 as date) < 18627)], Format: CSV, Location: InMemoryFileIndex[file:/home/lab12/src/data/trips/yellow_tripdata_2021-01.csv, file:/home/lab12/s..., PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "   :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#584]\n",
      "   :        +- *(1) Filter isnotnull(LocationID#92)\n",
      "   :           +- FileScan csv [LocationID#92] Batched: false, DataFilters: [isnotnull(LocationID#92)], Format: CSV, Location: InMemoryFileIndex[file:/home/lab12/src/data/taxi+_zone_lookup.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<LocationID:int>\n",
      "   +- ReusedExchange [LocationID#106], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#584]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e04793fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [cast(tpep_pickup_datetime#41 as date) AS pickup_date#100, hour(cast(tpep_pickup_datetime#41 as timestamp), Some(Asia/Seoul)) AS pickup_time#102]\n",
      "+- *(3) BroadcastHashJoin [DOLocationID#48], [LocationID#106], LeftOuter, BuildRight, false\n",
      "   :- *(3) Project [tpep_pickup_datetime#41, DOLocationID#48]\n",
      "   :  +- *(3) BroadcastHashJoin [PULocationID#47], [LocationID#92], LeftOuter, BuildRight, false\n",
      "   :     :- *(3) Filter ((isnotnull(tpep_pickup_datetime#41) AND (hour(cast(tpep_pickup_datetime#41 as timestamp), Some(Asia/Seoul)) > 0)) AND (hour(cast(tpep_pickup_datetime#41 as timestamp), Some(Asia/Seoul)) <= 12))\n",
      "   :     :  +- FileScan csv [tpep_pickup_datetime#41,PULocationID#47,DOLocationID#48] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#41), (hour(cast(tpep_pickup_datetime#41 as timestamp), Some(Asia/..., Format: CSV, Location: InMemoryFileIndex[file:/home/lab12/src/data/trips/yellow_tripdata_2021-01.csv, file:/home/lab12/s..., PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "   :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#668]\n",
      "   :        +- *(1) Filter isnotnull(LocationID#92)\n",
      "   :           +- FileScan csv [LocationID#92] Batched: false, DataFilters: [isnotnull(LocationID#92)], Format: CSV, Location: InMemoryFileIndex[file:/home/lab12/src/data/taxi+_zone_lookup.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<LocationID:int>\n",
      "   +- ReusedExchange [LocationID#106], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#668]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#실행계획, 실행결과(4040)\n",
    "\n",
    "query2 = '''\n",
    "select pickup_date, pickup_time \n",
    "from comb \n",
    "where pickup_time > 0 and pickup_time<=12\n",
    "'''\n",
    "spark.sql(query2).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2facf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Sort [pickup_date#100 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(pickup_date#100 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#816]\n",
      "   +- *(4) HashAggregate(keys=[pickup_date#100], functions=[count(1)])\n",
      "      +- Exchange hashpartitioning(pickup_date#100, 200), ENSURE_REQUIREMENTS, [id=#812]\n",
      "         +- *(3) HashAggregate(keys=[pickup_date#100], functions=[partial_count(1)])\n",
      "            +- *(3) Project [cast(tpep_pickup_datetime#41 as date) AS pickup_date#100]\n",
      "               +- *(3) BroadcastHashJoin [DOLocationID#48], [LocationID#106], LeftOuter, BuildRight, false\n",
      "                  :- *(3) Project [tpep_pickup_datetime#41, DOLocationID#48]\n",
      "                  :  +- *(3) BroadcastHashJoin [PULocationID#47], [LocationID#92], LeftOuter, BuildRight, false\n",
      "                  :     :- *(3) Filter (isnotnull(tpep_pickup_datetime#41) AND (hour(cast(tpep_pickup_datetime#41 as timestamp), Some(Asia/Seoul)) > 0))\n",
      "                  :     :  +- FileScan csv [tpep_pickup_datetime#41,PULocationID#47,DOLocationID#48] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#41), (hour(cast(tpep_pickup_datetime#41 as timestamp), Some(Asia/..., Format: CSV, Location: InMemoryFileIndex[file:/home/lab12/src/data/trips/yellow_tripdata_2021-01.csv, file:/home/lab12/s..., PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#773]\n",
      "                  :        +- *(1) Filter isnotnull(LocationID#92)\n",
      "                  :           +- FileScan csv [LocationID#92] Batched: false, DataFilters: [isnotnull(LocationID#92)], Format: CSV, Location: InMemoryFileIndex[file:/home/lab12/src/data/taxi+_zone_lookup.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<LocationID:int>\n",
      "                  +- ReusedExchange [LocationID#106], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#773]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3 = '''\n",
    "select pickup_date , count(*) as trip_count\n",
    "from comb \n",
    "where pickup_time > 0\n",
    "group by pickup_date\n",
    "order by pickup_date\n",
    "'''\n",
    "spark.sql(query3).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe451e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.운행 거리와 요금의 상관 관계 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fed597be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=============================================>        (170 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+---------+\n",
      "|pickup_hour|avg_fare|avg_tip|avg_total|\n",
      "+-----------+--------+-------+---------+\n",
      "|          0|   15.07|   2.49|    21.43|\n",
      "|          1|   14.67|   2.39|    20.84|\n",
      "|          2|   14.35|   2.27|    20.36|\n",
      "|          3|   15.06|   2.25|    21.07|\n",
      "|          4|   19.03|   2.40|    25.54|\n",
      "|          5|   19.81|   2.42|    26.55|\n",
      "|          6|   15.43|   2.16|    21.33|\n",
      "|          7|   13.29|   2.06|    18.84|\n",
      "|          8|   12.34|   2.05|    17.79|\n",
      "|          9|   12.21|   1.98|    17.59|\n",
      "|         10|   12.21|   1.93|    17.54|\n",
      "|         11|   12.15|   1.93|    17.47|\n",
      "|         12|   12.61|   1.95|    17.96|\n",
      "|         13|   12.75|   1.99|    18.15|\n",
      "|         14|   12.71|   2.05|    18.18|\n",
      "|         15|   13.05|   2.11|    18.63|\n",
      "|         16|   13.30|   2.24|    19.71|\n",
      "|         17|   12.88|   2.26|    19.29|\n",
      "|         18|   12.21|   2.25|    18.58|\n",
      "|         19|   12.03|   2.22|    18.36|\n",
      "|         20|   12.43|   2.24|    18.54|\n",
      "|         21|   12.99|   2.37|    19.24|\n",
      "|         22|   13.61|   2.44|    19.94|\n",
      "|         23|   14.50|   2.47|    20.86|\n",
      "+-----------+--------+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#2. 피크 시간대 요금 분석\n",
    "from pyspark.sql.functions import hour, avg, col, format_number\n",
    "\n",
    "# 1. Pickup 시간의 시간대(hour) 추출\n",
    "trips_with_time = trips_df.withColumn(\"pickup_hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# 2. 시간대별 평균 요금 계산\n",
    "fare_analysis_by_hour = trips_with_time.groupBy(\"pickup_hour\") \\\n",
    "    .agg(\n",
    "        avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "        avg(\"tip_amount\").alias(\"avg_tip\"),\n",
    "        avg(\"total_amount\").alias(\"avg_total\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_fare\", format_number(\"avg_fare\", 2)) \\\n",
    "    .withColumn(\"avg_tip\", format_number(\"avg_tip\", 2)) \\\n",
    "    .withColumn(\"avg_total\", format_number(\"avg_total\", 2)) \\\n",
    "    .orderBy(\"pickup_hour\")\n",
    "\n",
    "# 3. 결과 출력\n",
    "fare_analysis_by_hour.show(24)  # 24시간 시간대를 출력\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9740672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===============================================>      (176 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+---------+\n",
      "|pickup_hour|avg_fare|avg_tip|avg_total|\n",
      "+-----------+--------+-------+---------+\n",
      "|          0|   15.07|   2.49|    21.43|\n",
      "|          1|   14.67|   2.39|    20.84|\n",
      "|          2|   14.35|   2.27|    20.36|\n",
      "|          3|   15.06|   2.25|    21.07|\n",
      "|          4|   19.03|    2.4|    25.54|\n",
      "|          5|   19.81|   2.42|    26.55|\n",
      "|          6|   15.43|   2.16|    21.33|\n",
      "|          7|   13.29|   2.06|    18.84|\n",
      "|          8|   12.34|   2.05|    17.79|\n",
      "|          9|   12.21|   1.98|    17.59|\n",
      "|         10|   12.21|   1.93|    17.54|\n",
      "|         11|   12.15|   1.93|    17.47|\n",
      "|         12|   12.61|   1.95|    17.96|\n",
      "|         13|   12.75|   1.99|    18.15|\n",
      "|         14|   12.71|   2.05|    18.18|\n",
      "|         15|   13.05|   2.11|    18.63|\n",
      "|         16|    13.3|   2.24|    19.71|\n",
      "|         17|   12.88|   2.26|    19.29|\n",
      "|         18|   12.21|   2.25|    18.58|\n",
      "|         19|   12.03|   2.22|    18.36|\n",
      "|         20|   12.43|   2.24|    18.54|\n",
      "|         21|   12.99|   2.37|    19.24|\n",
      "|         22|   13.61|   2.44|    19.94|\n",
      "|         23|    14.5|   2.47|    20.86|\n",
      "+-----------+--------+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "\n",
    "# 1. Pickup 시간의 시간대(hour) 추출\n",
    "trips_with_time = trips_df.withColumn(\"pickup_hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# 2. TempView로 등록\n",
    "trips_with_time.createOrReplaceTempView(\"trips\")\n",
    "\n",
    "# 3. SQL 쿼리 실행\n",
    "fare_analysis_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        pickup_hour,\n",
    "        ROUND(AVG(fare_amount), 2) AS avg_fare,\n",
    "        ROUND(AVG(tip_amount), 2) AS avg_tip,\n",
    "        ROUND(AVG(total_amount), 2) AS avg_total\n",
    "    FROM trips\n",
    "    GROUP BY pickup_hour\n",
    "    ORDER BY pickup_hour\n",
    "\"\"\")\n",
    "\n",
    "# 4. 결과 출력\n",
    "fare_analysis_sql.show(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c0614c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:===============================================>      (175 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+---------+\n",
      "|pickup_hour|avg_fare|avg_tip|avg_total|\n",
      "+-----------+--------+-------+---------+\n",
      "|          5|   19.81|   2.42|    26.55|\n",
      "|          4|   19.03|    2.4|    25.54|\n",
      "|          0|   15.07|   2.49|    21.43|\n",
      "+-----------+--------+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#2.1 가장 요금이 비싼 시간대 탑3 추출\n",
    "# Top 3 시간대 추출 쿼리 실행\n",
    "top3_expensive_hours = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        pickup_hour,\n",
    "        ROUND(AVG(fare_amount), 2) AS avg_fare,\n",
    "        ROUND(AVG(tip_amount), 2) AS avg_tip,\n",
    "        ROUND(AVG(total_amount), 2) AS avg_total\n",
    "    FROM trips\n",
    "    GROUP BY pickup_hour\n",
    "    ORDER BY avg_total DESC\n",
    "    LIMIT 3\n",
    "\"\"\")\n",
    "\n",
    "# 결과 출력\n",
    "top3_expensive_hours.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 지불 유형별 요금 , 팁 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. 승차지역/ 하차지역별 평균거리, 요금"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fd4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. 팁의 비율에 따라 거리, 여행 건수 서비스 관련 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4be72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a57791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
