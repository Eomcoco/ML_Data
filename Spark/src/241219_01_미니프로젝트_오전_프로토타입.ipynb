{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f660bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 10:15:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------+------+----+--------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "|bib|name                |country       |gender|div |div_rank|overall_time|overall_rank|swim_time|swim_rank|bike_time|bike_rank|run_time|run_rank|finish_status|\n",
      "+---+--------------------+--------------+------+----+--------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "|8  |Gustav Iden         |Norway        |Male  |MPRO|1       |7:40:24     |1           |48:23:00 |10       |4:11:06  |6        |2:36:15 |1       |Finisher     |\n",
      "|15 |Sam Laidlow         |France        |Male  |MPRO|2       |7:42:24     |2           |48:16:00 |2        |4:04:36  |1        |2:44:40 |5       |Finisher     |\n",
      "|1  |Kristian Blummenfelt|Norway        |Male  |MPRO|3       |7:43:23     |3           |48:20:00 |5        |4:11:16  |8        |2:39:21 |2       |Finisher     |\n",
      "|23 |Max Neumann         |Australia     |Male  |MPRO|4       |7:44:44     |4           |48:25:00 |13       |4:11:30  |9        |2:40:14 |3       |Finisher     |\n",
      "|17 |Joe Skipper         |United Kingdom|Male  |MPRO|5       |7:54:05     |5           |52:55:00 |60       |4:11:11  |7        |2:45:26 |6       |Finisher     |\n",
      "|7  |Sebastian Kienle    |Germany       |Male  |MPRO|6       |7:55:40     |6           |52:58:00 |66       |4:09:11  |4        |2:48:45 |13      |Finisher     |\n",
      "|12 |Leon Chevalier      |France        |Male  |MPRO|7       |7:55:52     |7           |52:54:00 |59       |4:09:05  |3        |2:49:28 |15      |Finisher     |\n",
      "|32 |Magnus Ditlev       |Denmark       |Male  |MPRO|8       |7:56:38     |8           |49:49:00 |32       |4:13:38  |11       |2:48:11 |11      |Finisher     |\n",
      "|38 |Clement Mignon      |France        |Male  |MPRO|9       |7:56:58     |9           |49:50:00 |33       |4:15:14  |14       |2:46:00 |8       |Finisher     |\n",
      "|6  |Patrick Lange       |Germany       |Male  |MPRO|10      |7:58:20     |10          |49:42:00 |26       |4:21:52  |22       |2:41:59 |4       |Finisher     |\n",
      "|11 |Cameron Wurf        |Australia     |Male  |MPRO|11      |8:00:51     |11          |52:51:00 |56       |4:09:04  |2        |2:54:27 |19      |Finisher     |\n",
      "|5  |Florian Angert      |Germany       |Male  |MPRO|12      |8:01:53     |12          |48:15:00 |1        |4:17:58  |19       |2:50:29 |16      |Finisher     |\n",
      "|9  |Timothy O'Donnell   |United States |Male  |MPRO|13      |8:02:58     |13          |48:23:00 |11       |4:13:30  |10       |2:56:03 |21      |Finisher     |\n",
      "|21 |Denis Chevrot       |France        |Male  |MPRO|14      |8:03:24     |14          |48:26:00 |16       |4:22:59  |25       |2:47:03 |9       |Finisher     |\n",
      "|20 |Matthew Hanson      |United States |Male  |MPRO|15      |8:04:55     |15          |52:40:00 |51       |4:22:18  |24       |2:45:34 |7       |Finisher     |\n",
      "|44 |Mathias Petersen    |Denmark       |Male  |MPRO|16      |8:06:45     |16          |48:25:00 |14       |4:24:55  |31       |2:48:16 |12      |Finisher     |\n",
      "|33 |Bradley Weiss       |South Africa  |Male  |MPRO|17      |8:07:28     |17          |49:41:00 |25       |4:24:49  |30       |2:48:01 |10      |Finisher     |\n",
      "|47 |Luciano Taccone     |Argentina     |Male  |MPRO|18      |8:09:10     |18          |49:47:00 |30       |4:25:08  |33       |2:49:18 |14      |Finisher     |\n",
      "|52 |Henrik Goesch       |Finland       |Male  |MPRO|19      |8:10:25     |19          |49:47:00 |31       |4:21:57  |23       |2:53:49 |18      |Finisher     |\n",
      "|19 |Rudy Von Berg       |United States |Male  |MPRO|20      |8:12:47     |20          |49:43:00 |28       |4:15:24  |15       |3:02:17 |34      |Finisher     |\n",
      "+---+--------------------+--------------+------+----+--------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- bib: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- div: string (nullable = true)\n",
      " |-- div_rank: integer (nullable = true)\n",
      " |-- overall_time: string (nullable = true)\n",
      " |-- overall_rank: integer (nullable = true)\n",
      " |-- swim_time: string (nullable = true)\n",
      " |-- swim_rank: integer (nullable = true)\n",
      " |-- bike_time: string (nullable = true)\n",
      " |-- bike_rank: integer (nullable = true)\n",
      " |-- run_time: string (nullable = true)\n",
      " |-- run_rank: integer (nullable = true)\n",
      " |-- finish_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark 세션 초기화\n",
    "spark = SparkSession.builder.appName(\"Ironman Data Analysis_241219_01\").getOrCreate()\n",
    "\n",
    "# CSV 다시 불러오기\n",
    "file_path = \"file:///home/lab12/src/data/ironman_wc_2022.csv\"  # 정확한 경로 입력\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# 데이터 확인\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f35175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+---------------+---+\n",
      "|swim_seconds|bike_seconds|run_seconds|overall_seconds|DNF|\n",
      "+------------+------------+-----------+---------------+---+\n",
      "|      174180|       15066|       9375|          27624|  1|\n",
      "|      173760|       14676|       9880|          27744|  1|\n",
      "|      174000|       15076|       9561|          27803|  1|\n",
      "|      174300|       15090|       9614|          27884|  1|\n",
      "|      190500|       15071|       9926|          28445|  1|\n",
      "|      190680|       14951|      10125|          28540|  1|\n",
      "|      190440|       14945|      10168|          28552|  1|\n",
      "|      179340|       15218|      10091|          28598|  1|\n",
      "|      179400|       15314|       9960|          28618|  1|\n",
      "|      178920|       15712|       9719|          28700|  1|\n",
      "|      190260|       14944|      10467|          28851|  1|\n",
      "|      173700|       15478|      10229|          28913|  1|\n",
      "|      174180|       15210|      10563|          28978|  1|\n",
      "|      174360|       15779|      10023|          29004|  1|\n",
      "|      189600|       15738|       9934|          29095|  1|\n",
      "|      174300|       15895|      10096|          29205|  1|\n",
      "|      178860|       15889|      10081|          29248|  1|\n",
      "|      179220|       15908|      10158|          29350|  1|\n",
      "|      179220|       15717|      10429|          29425|  1|\n",
      "|      178980|       15324|      10937|          29567|  1|\n",
      "+------------+------------+-----------+---------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col, split\n",
    "\n",
    "# 1. 시간 데이터를 초 단위로 변환하는 함수 정의\n",
    "def time_to_seconds(time_str_col):\n",
    "    \"\"\"\n",
    "    문자열 형태의 시간 데이터를 초 단위로 변환\n",
    "    \"\"\"\n",
    "    return (split(time_str_col, \":\")[0].cast(\"int\") * 3600 +\n",
    "            split(time_str_col, \":\")[1].cast(\"int\") * 60 +\n",
    "            split(time_str_col, \":\")[2].cast(\"int\"))\n",
    "\n",
    "# 2. 초 단위 컬럼 생성\n",
    "df = df.withColumn(\"swim_seconds\", time_to_seconds(col(\"swim_time\"))) \\\n",
    "       .withColumn(\"bike_seconds\", time_to_seconds(col(\"bike_time\"))) \\\n",
    "       .withColumn(\"run_seconds\", time_to_seconds(col(\"run_time\"))) \\\n",
    "       .withColumn(\"overall_seconds\", time_to_seconds(col(\"overall_time\")))\n",
    "\n",
    "# 3. 컷오프 기준에 따른 DNF 컬럼 생성\n",
    "df = df.withColumn(\n",
    "    \"DNF\",\n",
    "    when((col(\"swim_seconds\") > 2 * 3600 + 20 * 60) |  # 수영 컷오프\n",
    "         (col(\"swim_seconds\") + col(\"bike_seconds\") > 10 * 3600 + 30 * 60) |  # 수영 + 사이클 컷오프\n",
    "         (col(\"overall_seconds\") > 17 * 3600), 1).otherwise(0)  # 전체 컷오프\n",
    ")\n",
    "\n",
    "# 4. 결과 확인\n",
    "df.select(\"swim_seconds\", \"bike_seconds\", \"run_seconds\", \"overall_seconds\", \"DNF\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67330fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter((col(\"finish_status\") != \"DNS\") & (col(\"finish_status\") != \"DQ\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5817c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"swim_seconds\", \"bike_seconds\", \"run_seconds\", \"overall_seconds\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590e2498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"swim_seconds\", col(\"swim_seconds\").cast(\"double\")) \\\n",
    "       .withColumn(\"bike_seconds\", col(\"bike_seconds\").cast(\"double\")) \\\n",
    "       .withColumn(\"run_seconds\", col(\"run_seconds\").cast(\"double\")) \\\n",
    "       .withColumn(\"overall_seconds\", col(\"overall_seconds\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab87d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|DNF|count|\n",
      "+---+-----+\n",
      "|  1|  345|\n",
      "|  0| 2031|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"DNF\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc97c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|DNF|count|\n",
      "+---+-----+\n",
      "|  0| 2031|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_finishers = df.filter(col(\"DNF\") == 0)\n",
    "df_finishers.select(\"DNF\").groupBy(\"DNF\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "423d0125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|gender|gender_encoded|\n",
      "+------+--------------+\n",
      "|  Male|           0.0|\n",
      "|  Male|           0.0|\n",
      "|  Male|           0.0|\n",
      "|  Male|           0.0|\n",
      "|  Male|           0.0|\n",
      "+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# gender 컬럼 인코딩\n",
    "indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_encoded\")\n",
    "df_finishers = indexer.fit(df_finishers).transform(df_finishers)\n",
    "\n",
    "# 결과 확인\n",
    "df_finishers.select(\"gender\", \"gender_encoded\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88b81e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|   div|age_group|\n",
      "+------+---------+\n",
      "|M30-34|       30|\n",
      "|M25-29|       25|\n",
      "|M18-24|       18|\n",
      "|M35-39|       35|\n",
      "|M40-44|       40|\n",
      "|M55-59|       55|\n",
      "|M45-49|       45|\n",
      "|M50-54|       50|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# div 컬럼을 기반으로 age_group 생성\n",
    "df_finishers = df_finishers.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"div\").startswith(\"M18-24\"), 18)\n",
    "    .when(col(\"div\").startswith(\"M25-29\"), 25)\n",
    "    .when(col(\"div\").startswith(\"M30-34\"), 30)\n",
    "    .when(col(\"div\").startswith(\"M35-39\"), 35)\n",
    "    .when(col(\"div\").startswith(\"M40-44\"), 40)\n",
    "    .when(col(\"div\").startswith(\"M45-49\"), 45)\n",
    "    .when(col(\"div\").startswith(\"M50-54\"), 50)\n",
    "    .when(col(\"div\").startswith(\"M55-59\"), 55)\n",
    "    .when(col(\"div\").startswith(\"MPRO\"), 0)  # 프로 선수\n",
    "    .otherwise(None)  # 나머지 경우 처리\n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "df_finishers.select(\"div\", \"age_group\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd0ecb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----------+\n",
      "|features                         |rank_range|\n",
      "+---------------------------------+----------+\n",
      "|[0.0,30.0,3655.0,16953.0,11146.0]|Top 10%   |\n",
      "|[0.0,30.0,3983.0,17318.0,10560.0]|Top 10%   |\n",
      "|[0.0,30.0,3755.0,17022.0,11094.0]|Top 10%   |\n",
      "|[0.0,35.0,4042.0,16196.0,11461.0]|Top 10%   |\n",
      "|[0.0,30.0,3955.0,17532.0,10395.0]|Top 10%   |\n",
      "+---------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# 1. 순위 그룹 라벨링\n",
    "total_participants = df_finishers.count()\n",
    "top_10 = total_participants * 0.1\n",
    "top_25 = total_participants * 0.25\n",
    "top_50 = total_participants * 0.5\n",
    "\n",
    "df_finishers = df_finishers.withColumn(\n",
    "    \"rank_range\",\n",
    "    when(col(\"overall_rank\") <= top_10, \"Top 10%\")\n",
    "    .when((col(\"overall_rank\") > top_10) & (col(\"overall_rank\") <= top_25), \"Top 25%\")\n",
    "    .when((col(\"overall_rank\") > top_25) & (col(\"overall_rank\") <= top_50), \"Top 50%\")\n",
    "    .otherwise(\"Bottom 50%\")\n",
    ")\n",
    "\n",
    "# 2. 피처 벡터 생성\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_final = assembler.transform(df_finishers).select(\"features\", \"rank_range\")\n",
    "\n",
    "# 결과 확인\n",
    "df_final.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be85b2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 크기: 1671\n",
      "테스트 데이터 크기: 360\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할\n",
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(\"훈련 데이터 크기:\", train_data.count())\n",
    "print(\"테스트 데이터 크기:\", test_data.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93e10078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rank_range: string (nullable = false)\n",
      "\n",
      "+---------------------------------+----------+\n",
      "|features                         |rank_range|\n",
      "+---------------------------------+----------+\n",
      "|[0.0,18.0,3653.0,20368.0,12874.0]|Top 50%   |\n",
      "|[0.0,18.0,3693.0,19738.0,17137.0]|Bottom 50%|\n",
      "|[0.0,18.0,3722.0,19532.0,17446.0]|Bottom 50%|\n",
      "|[0.0,18.0,3729.0,18523.0,14229.0]|Top 50%   |\n",
      "|[0.0,18.0,3732.0,18024.0,16481.0]|Bottom 50%|\n",
      "+---------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rank_range: string (nullable = false)\n",
      "\n",
      "+---------------------------------+----------+\n",
      "|features                         |rank_range|\n",
      "+---------------------------------+----------+\n",
      "|[0.0,18.0,3717.0,19283.0,13253.0]|Top 50%   |\n",
      "|[0.0,18.0,3776.0,18633.0,12743.0]|Top 50%   |\n",
      "|[0.0,18.0,3855.0,19772.0,14458.0]|Bottom 50%|\n",
      "|[0.0,18.0,3943.0,18864.0,14091.0]|Top 50%   |\n",
      "|[0.0,18.0,4134.0,18740.0,17225.0]|Bottom 50%|\n",
      "+---------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_data 스키마 및 데이터 확인\n",
    "train_data.printSchema()\n",
    "train_data.show(5, truncate=False)\n",
    "\n",
    "# test_data 스키마 및 데이터 확인\n",
    "test_data.printSchema()\n",
    "test_data.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9878a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|rank_range|rank_range_index|\n",
      "+----------+----------------+\n",
      "|   Top 10%|             3.0|\n",
      "|   Top 25%|             2.0|\n",
      "|Bottom 50%|             0.0|\n",
      "|   Top 50%|             1.0|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 기존 rank_range_index가 있다면 삭제\n",
    "if \"rank_range_index\" in df_final.columns:\n",
    "    df_final = df_final.drop(\"rank_range_index\")\n",
    "\n",
    "# rank_range를 숫자형 rank_range_index로 변환\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"rank_range\", outputCol=\"rank_range_index\")\n",
    "df_final = indexer.fit(df_final).transform(df_final)\n",
    "\n",
    "# 결과 확인\n",
    "df_final.select(\"rank_range\", \"rank_range_index\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27264e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rank_range: string (nullable = false)\n",
      " |-- rank_range_index: double (nullable = false)\n",
      "\n",
      "+---------------------------------+----------+----------------+\n",
      "|features                         |rank_range|rank_range_index|\n",
      "+---------------------------------+----------+----------------+\n",
      "|[0.0,18.0,3653.0,20368.0,12874.0]|Top 50%   |1.0             |\n",
      "|[0.0,18.0,3693.0,19738.0,17137.0]|Bottom 50%|0.0             |\n",
      "|[0.0,18.0,3722.0,19532.0,17446.0]|Bottom 50%|0.0             |\n",
      "|[0.0,18.0,3729.0,18523.0,14229.0]|Top 50%   |1.0             |\n",
      "|[0.0,18.0,3732.0,18024.0,16481.0]|Bottom 50%|0.0             |\n",
      "+---------------------------------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할\n",
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 데이터 확인\n",
    "train_data.printSchema()\n",
    "train_data.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66e94f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 크기: 1671\n",
      "테스트 데이터 크기: 360\n",
      "+---------------------------------+----------+----------------+\n",
      "|features                         |rank_range|rank_range_index|\n",
      "+---------------------------------+----------+----------------+\n",
      "|[0.0,18.0,3653.0,20368.0,12874.0]|Top 50%   |1.0             |\n",
      "|[0.0,18.0,3693.0,19738.0,17137.0]|Bottom 50%|0.0             |\n",
      "|[0.0,18.0,3722.0,19532.0,17446.0]|Bottom 50%|0.0             |\n",
      "|[0.0,18.0,3729.0,18523.0,14229.0]|Top 50%   |1.0             |\n",
      "|[0.0,18.0,3732.0,18024.0,16481.0]|Bottom 50%|0.0             |\n",
      "+---------------------------------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할\n",
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(\"훈련 데이터 크기:\", train_data.count())\n",
    "print(\"테스트 데이터 크기:\", test_data.count())\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "train_data.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8e92f8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----------------+----------+----------------------------------------------------------------------------------+\n",
      "|features                         |rank_range_index|prediction|probability                                                                       |\n",
      "+---------------------------------+----------------+----------+----------------------------------------------------------------------------------+\n",
      "|[0.0,18.0,3717.0,19283.0,13253.0]|1.0             |1.0       |[0.19375110197035592,0.7694830196427098,0.03667198167331931,9.389671361502349E-5] |\n",
      "|[0.0,18.0,3776.0,18633.0,12743.0]|1.0             |1.0       |[0.06460296006524288,0.5638580102696178,0.3352770956312607,0.03626193403387875]   |\n",
      "|[0.0,18.0,3855.0,19772.0,14458.0]|0.0             |0.0       |[0.8717716701471933,0.1264885405067432,0.0017397893460634995,0.0]                 |\n",
      "|[0.0,18.0,3943.0,18864.0,14091.0]|1.0             |1.0       |[0.3335679328557881,0.6208305351463654,0.045434865331179665,1.6666666666666663E-4]|\n",
      "|[0.0,18.0,4134.0,18740.0,17225.0]|0.0             |0.0       |[0.9923019914584615,0.007698008541538486,0.0,0.0]                                 |\n",
      "|[0.0,18.0,4212.0,18675.0,13225.0]|1.0             |1.0       |[0.10431991936599466,0.8080685716542495,0.08641344079393964,0.0011980681858162355]|\n",
      "|[0.0,18.0,4340.0,17468.0,13672.0]|1.0             |1.0       |[0.0666478541819575,0.676163152207631,0.24214629067731822,0.01504270293309328]    |\n",
      "|[0.0,18.0,4702.0,20051.0,13990.0]|0.0             |0.0       |[0.9644037426169139,0.0355962573830861,0.0,0.0]                                   |\n",
      "|[0.0,30.0,3600.0,19602.0,14738.0]|0.0             |0.0       |[0.9681022576695129,0.031897742330487054,0.0,0.0]                                 |\n",
      "|[0.0,30.0,3602.0,17998.0,13419.0]|1.0             |1.0       |[0.04225023605180593,0.6046364884892281,0.3278203373123372,0.02529293814662883]   |\n",
      "+---------------------------------+----------------+----------+----------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Random Forest 모델 생성 및 학습\n",
    "rf = RandomForestClassifier(labelCol=\"rank_range_index\", featuresCol=\"features\", numTrees=50)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# 테스트 데이터로 예측\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# 결과 확인\n",
    "predictions.select(\"features\", \"rank_range_index\", \"prediction\", \"probability\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea37a5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rank_range: string (nullable = false)\n",
      " |-- rank_range_index: double (nullable = false)\n",
      "\n",
      "+--------------------+----------+----------------+\n",
      "|            features|rank_range|rank_range_index|\n",
      "+--------------------+----------+----------------+\n",
      "|[0.0,18.0,3653.0,...|   Top 50%|             1.0|\n",
      "|[0.0,18.0,3693.0,...|Bottom 50%|             0.0|\n",
      "|[0.0,18.0,3722.0,...|Bottom 50%|             0.0|\n",
      "|[0.0,18.0,3729.0,...|   Top 50%|             1.0|\n",
      "|[0.0,18.0,3732.0,...|Bottom 50%|             0.0|\n",
      "+--------------------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터 스키마 확인\n",
    "train_data.printSchema()\n",
    "\n",
    "# 학습 데이터 상위 5개 확인\n",
    "train_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c4ca0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----------------+----------+----------------------------------------------------------------------------------+\n",
      "|features                         |rank_range_index|prediction|probability                                                                       |\n",
      "+---------------------------------+----------------+----------+----------------------------------------------------------------------------------+\n",
      "|[0.0,18.0,3717.0,19283.0,13253.0]|1.0             |1.0       |[0.19375110197035592,0.7694830196427098,0.03667198167331931,9.389671361502349E-5] |\n",
      "|[0.0,18.0,3776.0,18633.0,12743.0]|1.0             |1.0       |[0.06460296006524288,0.5638580102696178,0.3352770956312607,0.03626193403387875]   |\n",
      "|[0.0,18.0,3855.0,19772.0,14458.0]|0.0             |0.0       |[0.8717716701471933,0.1264885405067432,0.0017397893460634995,0.0]                 |\n",
      "|[0.0,18.0,3943.0,18864.0,14091.0]|1.0             |1.0       |[0.3335679328557881,0.6208305351463654,0.045434865331179665,1.6666666666666663E-4]|\n",
      "|[0.0,18.0,4134.0,18740.0,17225.0]|0.0             |0.0       |[0.9923019914584615,0.007698008541538486,0.0,0.0]                                 |\n",
      "|[0.0,18.0,4212.0,18675.0,13225.0]|1.0             |1.0       |[0.10431991936599466,0.8080685716542495,0.08641344079393964,0.0011980681858162355]|\n",
      "|[0.0,18.0,4340.0,17468.0,13672.0]|1.0             |1.0       |[0.0666478541819575,0.676163152207631,0.24214629067731822,0.01504270293309328]    |\n",
      "|[0.0,18.0,4702.0,20051.0,13990.0]|0.0             |0.0       |[0.9644037426169139,0.0355962573830861,0.0,0.0]                                   |\n",
      "|[0.0,30.0,3600.0,19602.0,14738.0]|0.0             |0.0       |[0.9681022576695129,0.031897742330487054,0.0,0.0]                                 |\n",
      "|[0.0,30.0,3602.0,17998.0,13419.0]|1.0             |1.0       |[0.04225023605180593,0.6046364884892281,0.3278203373123372,0.02529293814662883]   |\n",
      "+---------------------------------+----------------+----------+----------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Random Forest 모델 생성 및 학습\n",
    "rf = RandomForestClassifier(labelCol=\"rank_range_index\", featuresCol=\"features\", numTrees=50)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# 테스트 데이터로 예측\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# 결과 확인\n",
    "predictions.select(\"features\", \"rank_range_index\", \"prediction\", \"probability\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16bc7bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Actual Rank Predicted Rank  \\\n",
      "0     Top 50%        Top 50%   \n",
      "1     Top 50%        Top 50%   \n",
      "2  Bottom 50%     Bottom 50%   \n",
      "3     Top 50%        Top 50%   \n",
      "4  Bottom 50%     Bottom 50%   \n",
      "5     Top 50%        Top 50%   \n",
      "6     Top 50%        Top 50%   \n",
      "7  Bottom 50%     Bottom 50%   \n",
      "8  Bottom 50%     Bottom 50%   \n",
      "9     Top 50%        Top 50%   \n",
      "\n",
      "                                         Probability  \n",
      "0  Bottom 50%: 19.38%, Top 50%: 76.95%, Top 25%: ...  \n",
      "1  Bottom 50%: 6.46%, Top 50%: 56.39%, Top 25%: 3...  \n",
      "2  Bottom 50%: 87.18%, Top 50%: 12.65%, Top 25%: ...  \n",
      "3  Bottom 50%: 33.36%, Top 50%: 62.08%, Top 25%: ...  \n",
      "4  Bottom 50%: 99.23%, Top 50%: 0.77%, Top 25%: 0...  \n",
      "5  Bottom 50%: 10.43%, Top 50%: 80.81%, Top 25%: ...  \n",
      "6  Bottom 50%: 6.66%, Top 50%: 67.62%, Top 25%: 2...  \n",
      "7  Bottom 50%: 96.44%, Top 50%: 3.56%, Top 25%: 0...  \n",
      "8  Bottom 50%: 96.81%, Top 50%: 3.19%, Top 25%: 0...  \n",
      "9  Bottom 50%: 4.23%, Top 50%: 60.46%, Top 25%: 3...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Spark DataFrame -> Pandas DataFrame 변환\n",
    "result_pd = predictions.select(\"rank_range_index\", \"prediction\", \"probability\").toPandas()\n",
    "\n",
    "# 클래스 매핑 (숫자 -> 범위 이름)\n",
    "class_mapping = {\n",
    "    0.0: \"Bottom 50%\",\n",
    "    1.0: \"Top 50%\",\n",
    "    2.0: \"Top 25%\",\n",
    "    3.0: \"Top 10%\"\n",
    "}\n",
    "\n",
    "# 매핑 적용\n",
    "result_pd[\"Actual Rank\"] = result_pd[\"rank_range_index\"].map(class_mapping)\n",
    "result_pd[\"Predicted Rank\"] = result_pd[\"prediction\"].map(class_mapping)\n",
    "\n",
    "# 확률 정보 문자열로 포맷\n",
    "result_pd[\"Probability\"] = result_pd[\"probability\"].apply(\n",
    "    lambda x: f\"Bottom 50%: {x[0]:.2%}, Top 50%: {x[1]:.2%}, Top 25%: {x[2]:.2%}, Top 10%: {x[3]:.2%}\"\n",
    ")\n",
    "\n",
    "# 필요없는 컬럼 제거\n",
    "result_pd = result_pd[[\"Actual Rank\", \"Predicted Rank\", \"Probability\"]]\n",
    "\n",
    "# 결과 출력\n",
    "print(result_pd.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef77aeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted Rank                                        Probability\n",
      "0        Top 50%  Bottom 50%: 28.34%, Top 50%: 54.46%, Top 25%: ...\n",
      "1     Bottom 50%  Bottom 50%: 97.48%, Top 50%: 2.52%, Top 25%: 0...\n"
     ]
    }
   ],
   "source": [
    "# 사용자 입력 데이터 예시\n",
    "input_data = [\n",
    "    [1.0, 33.0, 7500.0, 16000.0, 14000.0],  # Gender, Age, Swim, Bike, Run\n",
    "    [0.0, 28.0, 9000.0, 18000.0, 17000.0]   # Example 2\n",
    "]\n",
    "\n",
    "# 입력 데이터 변환\n",
    "input_df = spark.createDataFrame(input_data, schema=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"])\n",
    "input_features = assembler.transform(input_df)\n",
    "\n",
    "# 예측 수행\n",
    "input_predictions = rf_model.transform(input_features)\n",
    "\n",
    "# 결과 출력 포맷\n",
    "input_result = input_predictions.select(\"features\", \"prediction\", \"probability\").toPandas()\n",
    "\n",
    "# 클래스 매핑 적용\n",
    "input_result[\"Predicted Rank\"] = input_result[\"prediction\"].map(class_mapping)\n",
    "input_result[\"Probability\"] = input_result[\"probability\"].apply(\n",
    "    lambda x: f\"Bottom 50%: {x[0]:.2%}, Top 50%: {x[1]:.2%}, Top 25%: {x[2]:.2%}, Top 10%: {x[3]:.2%}\"\n",
    ")\n",
    "\n",
    "# 최종 출력\n",
    "print(input_result[[\"Predicted Rank\", \"Probability\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32cf2bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터:\n",
      "- 성별: 남성\n",
      "- 나이: 33세\n",
      "- 기록:\n",
      "    - 수영 (3.8km): 01:07:54\n",
      "    - 자전거 (180km): 07:28:08\n",
      "    - 달리기 (42.2km): 03:37:24\n",
      "\n",
      "모델 예측 결과:\n",
      "- 예상 종합 순위 그룹: Bottom 50%\n",
      "- 예상 부문 순위 그룹 (성별: 남성, 나이 그룹: 30대): Bottom 50%\n",
      "\n",
      "종목별 개선 방향:\n",
      "    - Swim: 상위 10%에 진입하려면 1시간 5분 이하로 줄여야 합니다.\n",
      "    - Bike: 상위 10%에 진입하려면 6시간 0분 이하로 줄여야 합니다.\n",
      "    - Run: 현재 기록이 평균 이상이며 추가 개선은 필요하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "# 사용자 입력 데이터를 아이언맨 기준으로 변환하는 함수\n",
    "def convert_to_ironman_seconds(swim_time, bike_time, run_time):\n",
    "    \"\"\"\n",
    "    수영, 자전거, 달리기 기록을 입력받아 초 단위로 변환.\n",
    "    \"\"\"\n",
    "    def time_to_seconds(time_str):\n",
    "        h, m, s = map(int, time_str.split(\":\"))\n",
    "        return h * 3600 + m * 60 + s\n",
    "\n",
    "    swim_seconds = time_to_seconds(swim_time)\n",
    "    bike_seconds = time_to_seconds(bike_time)\n",
    "    run_seconds = time_to_seconds(run_time)\n",
    "    return swim_seconds, bike_seconds, run_seconds\n",
    "\n",
    "\n",
    "# 사용자 입력 데이터\n",
    "user_data = {\n",
    "    \"gender\": \"남성\",\n",
    "    \"age\": 33,\n",
    "    \"swim_time\": \"00:26:48\",  # 수영 1.5km\n",
    "    \"bike_time\": \"01:39:35\",  # 자전거 40km\n",
    "    \"run_time\": \"00:51:31\",   # 달리기 10km\n",
    "}\n",
    "\n",
    "# 입력 데이터를 변환\n",
    "swim_seconds, bike_seconds, run_seconds = convert_to_ironman_seconds(\n",
    "    user_data[\"swim_time\"], user_data[\"bike_time\"], user_data[\"run_time\"]\n",
    ")\n",
    "\n",
    "# 아이언맨 기준 변환 (1.5km -> 3.8km, 40km -> 180km, 10km -> 42.2km)\n",
    "swim_seconds = swim_seconds * (3.8 / 1.5)\n",
    "bike_seconds = bike_seconds * (180 / 40)\n",
    "run_seconds = run_seconds * (42.2 / 10)\n",
    "\n",
    "# 모델 입력 데이터 구성\n",
    "input_data = [[1.0, user_data[\"age\"], swim_seconds, bike_seconds, run_seconds]]\n",
    "input_df = spark.createDataFrame(input_data, schema=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"])\n",
    "\n",
    "# 입력 데이터 변환 및 예측\n",
    "input_features = assembler.transform(input_df)\n",
    "predictions = rf_model.transform(input_features)\n",
    "\n",
    "# 예측 결과 해석\n",
    "prediction = predictions.select(\"prediction\", \"probability\").collect()[0]\n",
    "predicted_rank = class_mapping[prediction[\"prediction\"]]\n",
    "probabilities = prediction[\"probability\"]\n",
    "\n",
    "# 개선 방향 분석\n",
    "improvement_guidelines = {\n",
    "    \"swim\": \"상위 10%에 진입하려면 1시간 5분 이하로 줄여야 합니다.\",\n",
    "    \"bike\": \"상위 10%에 진입하려면 6시간 0분 이하로 줄여야 합니다.\",\n",
    "    \"run\": \"현재 기록이 평균 이상이며 추가 개선은 필요하지 않습니다.\",\n",
    "}\n",
    "\n",
    "# 출력 결과 구성\n",
    "print(\"예상 데이터:\")\n",
    "print(f\"- 성별: {user_data['gender']}\")\n",
    "print(f\"- 나이: {user_data['age']}세\")\n",
    "print(f\"- 예상기록:\")\n",
    "print(f\"    - 수영 (3.8km): {swim_seconds // 3600:02.0f}:{(swim_seconds % 3600) // 60:02.0f}:{swim_seconds % 60:02.0f}\")\n",
    "print(f\"    - 자전거 (180km): {bike_seconds // 3600:02.0f}:{(bike_seconds % 3600) // 60:02.0f}:{bike_seconds % 60:02.0f}\")\n",
    "print(f\"    - 달리기 (42.2km): {run_seconds // 3600:02.0f}:{(run_seconds % 3600) // 60:02.0f}:{run_seconds % 60:02.0f}\")\n",
    "print(\"\\n모델 예측 결과:\")\n",
    "print(f\"- 예상 종합 순위 그룹: {predicted_rank}\")\n",
    "print(f\"- 예상 부문 순위 그룹 (M30-34): {predicted_rank}\")\n",
    "print(\"\\n종목별 개선 방향:\")\n",
    "for event, guideline in improvement_guidelines.items():\n",
    "    print(f\"    - {event.capitalize()}: {guideline}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d378826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터:\n",
      "- 성별: 남성\n",
      "- 나이: 33세\n",
      "- 기록:\n",
      "    - 수영 (3.8km): 01:07:54\n",
      "    - 자전거 (180km): 07:28:08\n",
      "    - 달리기 (42.2km): 03:37:24\n",
      "\n",
      "모델 예측 결과:\n",
      "- 예상 종합 순위 그룹: Bottom 50%\n",
      "- 예상 부문 순위 그룹 (성별: 남성, 나이 그룹: 30대): Bottom 50%\n",
      "\n",
      "종목별 개선 방향:\n",
      "    - Swim: 상위 10%에 진입하려면 1시간 5분 이하로 줄여야 합니다.\n",
      "    - Bike: 상위 10%에 진입하려면 6시간 0분 이하로 줄여야 합니다.\n",
      "    - Run: 현재 기록이 평균 이상이며 추가 개선은 필요하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "# 사용자 데이터\n",
    "user_data = {\n",
    "    \"gender\": \"남성\",\n",
    "    \"age\": 33,\n",
    "    \"swim_time\": \"00:26:48\",  # 수영 1.5km\n",
    "    \"bike_time\": \"01:39:35\",  # 자전거 40km\n",
    "    \"run_time\": \"00:51:31\",   # 달리기 10km\n",
    "}\n",
    "\n",
    "# 성별 변환\n",
    "gender_encoded = 1.0 if user_data[\"gender\"] == \"남성\" else 0.0\n",
    "\n",
    "# 시간 데이터를 초 단위로 변환 함수\n",
    "def time_to_seconds(time_str):\n",
    "    h, m, s = map(int, time_str.split(\":\"))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "# 입력 데이터를 초 단위로 변환 및 거리 변환\n",
    "swim_seconds = time_to_seconds(user_data[\"swim_time\"]) * (3.8 / 1.5)  # 수영 3.8km 기준 변환\n",
    "bike_seconds = time_to_seconds(user_data[\"bike_time\"]) * (180 / 40)  # 자전거 180km 기준 변환\n",
    "run_seconds = time_to_seconds(user_data[\"run_time\"]) * (42.2 / 10)  # 달리기 42.2km 기준 변환\n",
    "\n",
    "# 모델 입력 데이터 생성\n",
    "input_data = [[gender_encoded, user_data[\"age\"], swim_seconds, bike_seconds, run_seconds]]\n",
    "input_df = spark.createDataFrame(input_data, schema=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"])\n",
    "input_features = assembler.transform(input_df)\n",
    "\n",
    "# 모델 예측\n",
    "predictions = rf_model.transform(input_features)\n",
    "prediction = predictions.select(\"prediction\", \"probability\").collect()[0]\n",
    "predicted_rank = class_mapping[prediction[\"prediction\"]]\n",
    "probabilities = prediction[\"probability\"]\n",
    "\n",
    "# 개선 방향 분석 (예제)\n",
    "improvement_guidelines = {\n",
    "    \"swim\": \"상위 10%에 진입하려면 1시간 5분 이하로 줄여야 합니다.\",\n",
    "    \"bike\": \"상위 10%에 진입하려면 6시간 0분 이하로 줄여야 합니다.\",\n",
    "    \"run\": \"현재 기록이 평균 이상이며 추가 개선은 필요하지 않습니다.\",\n",
    "}\n",
    "\n",
    "# 결과 출력\n",
    "print(\"입력 데이터:\")\n",
    "print(f\"- 성별: {user_data['gender']}\")\n",
    "print(f\"- 나이: {user_data['age']}세\")\n",
    "print(f\"- 기록:\")\n",
    "print(f\"    - 수영 (3.8km): {swim_seconds // 3600:02.0f}:{(swim_seconds % 3600) // 60:02.0f}:{swim_seconds % 60:02.0f}\")\n",
    "print(f\"    - 자전거 (180km): {bike_seconds // 3600:02.0f}:{(bike_seconds % 3600) // 60:02.0f}:{bike_seconds % 60:02.0f}\")\n",
    "print(f\"    - 달리기 (42.2km): {run_seconds // 3600:02.0f}:{(run_seconds % 3600) // 60:02.0f}:{run_seconds % 60:02.0f}\")\n",
    "print(\"\\n모델 예측 결과:\")\n",
    "print(f\"- 예상 종합 순위 그룹: {predicted_rank}\")\n",
    "print(f\"- 예상 부문 순위 그룹 (성별: {user_data['gender']}, 나이 그룹: {user_data['age'] // 10 * 10}대): {predicted_rank}\")\n",
    "print(\"\\n종목별 개선 방향:\")\n",
    "for event, guideline in improvement_guidelines.items():\n",
    "    print(f\"    - {event.capitalize()}: {guideline}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b15c2b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아이언맨 기록을 입력해주세요:\n",
      "성별 (남성/여성): 여성\n",
      "나이: 37\n",
      "수영 기록 (hh:mm:ss, 1.5km 기준): 00:26:48\n",
      "자전거 기록 (hh:mm:ss, 40km 기준): 01:39:35\n",
      "달리기 기록 (hh:mm:ss, 10km 기준): 00:51:32\n",
      "\n",
      "모델 예측 결과:\n",
      "- 예상 종합 순위 그룹: Bottom 50%\n",
      "- 예상 부문 순위 그룹 (성별: 여성, 나이 그룹: 30대): Bottom 50%\n",
      "\n",
      "종목별 개선 방향:\n",
      "    - Swim: 상위 10%에 진입하려면 1시간 5분 이하로 줄여야 합니다.\n",
      "    - Bike: 상위 10%에 진입하려면 6시간 0분 이하로 줄여야 합니다.\n",
      "    - Run: 현재 기록이 평균 이상이며 추가 개선은 필요하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 임포트\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# 시간 데이터를 초 단위로 변환하는 함수\n",
    "def time_to_seconds(time_str):\n",
    "    h, m, s = map(int, time_str.split(\":\"))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "# 클래스 매핑\n",
    "class_mapping = {\n",
    "    0.0: \"Bottom 50%\",\n",
    "    1.0: \"Top 50%\",\n",
    "    2.0: \"Top 25%\",\n",
    "    3.0: \"Top 10%\"\n",
    "}\n",
    "\n",
    "# 개선 방향 예시\n",
    "improvement_guidelines = {\n",
    "    \"swim\": \"상위 10%에 진입하려면 1시간 5분 이하로 줄여야 합니다.\",\n",
    "    \"bike\": \"상위 10%에 진입하려면 6시간 0분 이하로 줄여야 합니다.\",\n",
    "    \"run\": \"현재 기록이 평균 이상이며 추가 개선은 필요하지 않습니다.\",\n",
    "}\n",
    "\n",
    "# 사용자 입력 데이터 받기\n",
    "print(\"아이언맨 기록을 입력해주세요:\")\n",
    "gender = input(\"성별 (남성/여성): \")\n",
    "age = int(input(\"나이: \"))\n",
    "swim_time = input(\"수영 기록 (hh:mm:ss, 1.5km 기준): \")\n",
    "bike_time = input(\"자전거 기록 (hh:mm:ss, 40km 기준): \")\n",
    "run_time = input(\"달리기 기록 (hh:mm:ss, 10km 기준): \")\n",
    "\n",
    "# 성별 인코딩 및 시간 변환\n",
    "gender_encoded = 1.0 if gender == \"남성\" else 0.0\n",
    "swim_seconds = time_to_seconds(swim_time) * (3.8 / 1.5)  # 수영 3.8km로 변환\n",
    "bike_seconds = time_to_seconds(bike_time) * (180 / 40)  # 자전거 180km로 변환\n",
    "run_seconds = time_to_seconds(run_time) * (42.2 / 10)  # 달리기 42.2km로 변환\n",
    "\n",
    "# 입력 데이터 생성\n",
    "input_data = [[gender_encoded, age, swim_seconds, bike_seconds, run_seconds]]\n",
    "input_df = spark.createDataFrame(input_data, schema=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"])\n",
    "input_features = assembler.transform(input_df)\n",
    "\n",
    "# 모델 예측\n",
    "predictions = rf_model.transform(input_features)\n",
    "prediction = predictions.select(\"prediction\", \"probability\").collect()[0]\n",
    "predicted_rank = class_mapping[prediction[\"prediction\"]]\n",
    "probabilities = prediction[\"probability\"]\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n모델 예측 결과:\")\n",
    "print(f\"- 예상 종합 순위 그룹: {predicted_rank}\")\n",
    "print(f\"- 예상 부문 순위 그룹 (성별: {gender}, 나이 그룹: {age // 10 * 10}대): {predicted_rank}\")\n",
    "print(\"\\n종목별 개선 방향:\")\n",
    "for event, guideline in improvement_guidelines.items():\n",
    "    print(f\"    - {event.capitalize()}: {guideline}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b265bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "443376c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아이언맨 기록을 입력해주세요:\n",
      "성별 (남성/여성): 여성\n",
      "나이: 37\n",
      "수영 기록 (hh:mm:ss, 1.5km 기준): 00:26:48\n",
      "자전거 기록 (hh:mm:ss, 40km 기준): 01:39:35\n",
      "달리기 기록 (hh:mm:ss, 10km 기준): 00:51:32\n",
      "\n",
      "모델 예측 결과:\n",
      "- 예상 종합 순위 그룹: Bottom 50%\n",
      "- 예상 부문 순위 그룹 (성별: 여성, 나이 그룹: 30대): Bottom 50%\n",
      "\n",
      "예상 기록:\n",
      "    - 수영: 01:30:00\n",
      "    - 자전거: 06:30:00\n",
      "    - 달리기: 04:30:00\n",
      "\n",
      "종목별 개선 방향:\n",
      "    - 수영: 상위 10%에 진입하려면 1시간 5분 이하로 줄여야 합니다.\n",
      "    - 자전거: 상위 10%에 진입하려면 6시간 0분 이하로 줄여야 합니다.\n",
      "    - 달리기: 현재 기록이 평균 이상이며 추가 개선은 필요하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 임포트\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# 시간 데이터를 초 단위로 변환하는 함수\n",
    "def time_to_seconds(time_str):\n",
    "    h, m, s = map(int, time_str.split(\":\"))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "# 초를 hh:mm:ss 형식으로 변환하는 함수\n",
    "def seconds_to_time(seconds):\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = int(seconds % 60)\n",
    "    return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "# 클래스 매핑\n",
    "class_mapping = {\n",
    "    0.0: \"Bottom 50%\",\n",
    "    1.0: \"Top 50%\",\n",
    "    2.0: \"Top 25%\",\n",
    "    3.0: \"Top 10%\"\n",
    "}\n",
    "\n",
    "# 예상 기록 계산 함수\n",
    "def calculate_average_times(predicted_group):\n",
    "    avg_times = {\n",
    "        \"Bottom 50%\": {\"swim\": 1.5 * 3600, \"bike\": 6.5 * 3600, \"run\": 4.5 * 3600},\n",
    "        \"Top 50%\": {\"swim\": 1.4 * 3600, \"bike\": 6.0 * 3600, \"run\": 4.0 * 3600},\n",
    "        \"Top 25%\": {\"swim\": 1.3 * 3600, \"bike\": 5.5 * 3600, \"run\": 3.5 * 3600},\n",
    "        \"Top 10%\": {\"swim\": 1.2 * 3600, \"bike\": 5.0 * 3600, \"run\": 3.0 * 3600},\n",
    "    }\n",
    "    return avg_times[predicted_group]\n",
    "\n",
    "# 사용자 입력 데이터 받기\n",
    "print(\"자신의 기록을 입력해주세요:\")\n",
    "gender = input(\"성별 (남성/여성): \")\n",
    "age = int(input(\"나이: \"))\n",
    "swim_time = input(\"수영 기록 (hh:mm:ss, 1.5km 기준): \")\n",
    "bike_time = input(\"자전거 기록 (hh:mm:ss, 40km 기준): \")\n",
    "run_time = input(\"달리기 기록 (hh:mm:ss, 10km 기준): \")\n",
    "\n",
    "# 성별 인코딩 및 시간 변환\n",
    "gender_encoded = 1.0 if gender == \"남성\" else 0.0\n",
    "swim_seconds = time_to_seconds(swim_time) * (3.8 / 1.5)  # 수영 3.8km로 변환\n",
    "bike_seconds = time_to_seconds(bike_time) * (180 / 40)  # 자전거 180km로 변환\n",
    "run_seconds = time_to_seconds(run_time) * (42.2 / 10)  # 달리기 42.2km로 변환\n",
    "\n",
    "# 입력 데이터 생성\n",
    "input_data = [[gender_encoded, age, swim_seconds, bike_seconds, run_seconds]]\n",
    "input_df = spark.createDataFrame(input_data, schema=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"])\n",
    "input_features = assembler.transform(input_df)\n",
    "\n",
    "# 모델 예측\n",
    "predictions = rf_model.transform(input_features)\n",
    "prediction = predictions.select(\"prediction\", \"probability\").collect()[0]\n",
    "predicted_rank = class_mapping[prediction[\"prediction\"]]\n",
    "probabilities = prediction[\"probability\"]\n",
    "\n",
    "# 예상 기록 계산\n",
    "average_times = calculate_average_times(predicted_rank)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n모델 예측 결과:\")\n",
    "print(f\"- 예상 종합 순위 그룹: {predicted_rank}\")\n",
    "print(f\"- 예상 부문 순위 그룹 (성별: {gender}, 나이 그룹: {age // 10 * 10}대): {predicted_rank}\")\n",
    "\n",
    "print(\"\\n예상 기록:\")\n",
    "print(f\"    - 수영: {seconds_to_time(average_times['swim'])}\")\n",
    "print(f\"    - 자전거: {seconds_to_time(average_times['bike'])}\")\n",
    "print(f\"    - 달리기: {seconds_to_time(average_times['run'])}\")\n",
    "\n",
    "print(\"\\n종목별 개선 방향:\")\n",
    "print(\"    - 수영: 상위 10%에 진입하려면 1시간 5분 이하로 줄여야 합니다.\")\n",
    "print(\"    - 자전거: 상위 10%에 진입하려면 6시간 0분 이하로 줄여야 합니다.\")\n",
    "print(\"    - 달리기: 현재 기록이 평균 이상이며 추가 개선은 필요하지 않습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4feadea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c66bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef604880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d4970b3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`rank_range_index`' given input columns: [age_group, bike_seconds, features, gender_encoded, prediction, probability, rawPrediction, run_seconds, swim_seconds];\n'Project [features#2439, 'rank_range_index, prediction#2474, probability#2459]\n+- Project [gender_encoded#2428, age_group#2429, swim_seconds#2430, bike_seconds#2431, run_seconds#2432, features#2439, rawPrediction#2448, probability#2459, UDF(rawPrediction#2448) AS prediction#2474]\n   +- Project [gender_encoded#2428, age_group#2429, swim_seconds#2430, bike_seconds#2431, run_seconds#2432, features#2439, rawPrediction#2448, UDF(rawPrediction#2448) AS probability#2459]\n      +- Project [gender_encoded#2428, age_group#2429, swim_seconds#2430, bike_seconds#2431, run_seconds#2432, features#2439, UDF(features#2439) AS rawPrediction#2448]\n         +- Project [gender_encoded#2428, age_group#2429, swim_seconds#2430, bike_seconds#2431, run_seconds#2432, UDF(struct(gender_encoded, gender_encoded#2428, age_group, age_group#2429, swim_seconds, swim_seconds#2430, bike_seconds, bike_seconds#2431, run_seconds, run_seconds#2432)) AS features#2439]\n            +- LogicalRDD [gender_encoded#2428, age_group#2429, swim_seconds#2430, bike_seconds#2431, run_seconds#2432], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Spark DataFrame -> Pandas DataFrame 변환\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result_pd \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrank_range_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprobability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 클래스 매핑 (숫자 -> 범위 이름)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m class_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m0.0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBottom 50\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m1.0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 50\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m2.0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 25\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m3.0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m }\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1669\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols):\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \n\u001b[1;32m   1651\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1669\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`rank_range_index`' given input columns: [age_group, bike_seconds, features, gender_encoded, prediction, probability, rawPrediction, run_seconds, swim_seconds];\n'Project [features#2439, 'rank_range_index, prediction#2474, probability#2459]\n+- Project [gender_encoded#2428, age_group#2429, swim_seconds#2430, bike_seconds#2431, run_seconds#2432, features#2439, rawPrediction#2448, probability#2459, UDF(rawPrediction#2448) AS prediction#2474]\n   +- Project [gender_encoded#2428, age_group#2429, swim_seconds#2430, bike_seconds#2431, run_seconds#2432, features#2439, rawPrediction#2448, UDF(rawPrediction#2448) AS probability#2459]\n      +- Project [gender_encoded#2428, age_group#2429, swim_seconds#2430, bike_seconds#2431, run_seconds#2432, features#2439, UDF(features#2439) AS rawPrediction#2448]\n         +- Project [gender_encoded#2428, age_group#2429, swim_seconds#2430, bike_seconds#2431, run_seconds#2432, UDF(struct(gender_encoded, gender_encoded#2428, age_group, age_group#2429, swim_seconds, swim_seconds#2430, bike_seconds, bike_seconds#2431, run_seconds, run_seconds#2432)) AS features#2439]\n            +- LogicalRDD [gender_encoded#2428, age_group#2429, swim_seconds#2430, bike_seconds#2431, run_seconds#2432], false\n"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame -> Pandas DataFrame 변환\n",
    "result_pd = predictions.select(\"features\", \"rank_range_index\", \"prediction\", \"probability\").toPandas()\n",
    "\n",
    "# 클래스 매핑 (숫자 -> 범위 이름)\n",
    "class_mapping = {\n",
    "    0.0: \"Bottom 50%\",\n",
    "    1.0: \"Top 50%\",\n",
    "    2.0: \"Top 25%\",\n",
    "    3.0: \"Top 10%\"\n",
    "}\n",
    "\n",
    "# 매핑 적용\n",
    "result_pd[\"Actual Rank\"] = result_pd[\"rank_range_index\"].map(class_mapping)\n",
    "result_pd[\"Predicted Rank\"] = result_pd[\"prediction\"].map(class_mapping)\n",
    "\n",
    "# 확률 정보 문자열로 포맷\n",
    "result_pd[\"Probability\"] = result_pd[\"probability\"].apply(\n",
    "    lambda x: f\"Bottom 50%: {x[0]:.2%}, Top 50%: {x[1]:.2%}, Top 25%: {x[2]:.2%}, Top 10%: {x[3]:.2%}\"\n",
    ")\n",
    "\n",
    "# 필요 없는 컬럼 제거\n",
    "result_pd = result_pd[[\"Actual Rank\", \"Predicted Rank\", \"Probability\"]]\n",
    "\n",
    "# 결과 출력\n",
    "print(result_pd.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c52e7be1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 4. 예측 수행\u001b[39;00m\n\u001b[1;32m     38\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m  \u001b[38;5;66;03m# 학습된 RandomForest 모델 로드\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m(input_features)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 5. 결과 처리 및 출력\u001b[39;00m\n\u001b[1;32m     42\u001b[0m result \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# 변환 함수 정의\n",
    "def convert_to_ironman_standard(input_data):\n",
    "    swim_factor = 3.8 / 1.5\n",
    "    bike_factor = 180 / 40\n",
    "    run_factor = 42.2 / 10\n",
    "    converted_data = []\n",
    "    for row in input_data:\n",
    "        gender, age, swim, bike, run = row\n",
    "        converted_data.append([\n",
    "            gender,\n",
    "            age,\n",
    "            swim * swim_factor,\n",
    "            bike * bike_factor,\n",
    "            run * run_factor\n",
    "        ])\n",
    "    return converted_data\n",
    "\n",
    "# 1. Spark 세션 및 모델 로드\n",
    "spark = SparkSession.builder.appName(\"Ironman Prediction\").getOrCreate()\n",
    "\n",
    "# 2. Assembler 설정\n",
    "assembler = VectorAssembler(inputCols=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"], outputCol=\"features\")\n",
    "\n",
    "# 3. 사용자 데이터 변환\n",
    "input_data = [\n",
    "    [1.0, 33.0, 1500.0, 3600.0, 2400.0],\n",
    "    [0.0, 28.0, 1600.0, 4000.0, 2600.0]\n",
    "]\n",
    "converted_data = convert_to_ironman_standard(input_data)\n",
    "input_df = spark.createDataFrame(converted_data, schema=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"])\n",
    "input_features = assembler.transform(input_df)\n",
    "\n",
    "# 4. 예측 수행\n",
    "rf_model = ...  # 학습된 RandomForest 모델 로드\n",
    "predictions = rf_model.transform(input_features)\n",
    "\n",
    "# 5. 결과 처리 및 출력\n",
    "result = predictions.select(\"features\", \"prediction\", \"probability\").toPandas()\n",
    "class_mapping = {0.0: \"Bottom 50%\", 1.0: \"Top 50%\", 2.0: \"Top 25%\", 3.0: \"Top 10%\"}\n",
    "result[\"Predicted Rank\"] = result[\"prediction\"].map(class_mapping)\n",
    "result[\"Probability\"] = result[\"probability\"].apply(\n",
    "    lambda x: f\"Bottom 50%: {x[0]:.2%}, Top 50%: {x[1]:.2%}, Top 25%: {x[2]:.2%}, Top 10%: {x[3]:.2%}\"\n",
    ")\n",
    "print(result[[\"Predicted Rank\", \"Probability\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4cbda1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted Rank                                        Probability\n",
      "0        Top 50%  Bottom 50%: 28.34%, Top 50%: 54.46%, Top 25%: ...\n",
      "1     Bottom 50%  Bottom 50%: 97.48%, Top 50%: 2.52%, Top 25%: 0...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 사용자 데이터 예시\n",
    "input_data = [\n",
    "    [1.0, 33.0, 7500.0, 16000.0, 14000.0],  # Gender, Age, Swim, Bike, Run\n",
    "    [0.0, 28.0, 9000.0, 18000.0, 17000.0]\n",
    "]\n",
    "\n",
    "# 입력 데이터 변환\n",
    "input_df = spark.createDataFrame(input_data, schema=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"])\n",
    "input_features = assembler.transform(input_df)\n",
    "\n",
    "# 예측 수행\n",
    "predictions = rf_model.transform(input_features)\n",
    "\n",
    "# 결과 출력 포맷\n",
    "result = predictions.select(\"features\", \"prediction\", \"probability\").toPandas()\n",
    "\n",
    "# 결과 변환\n",
    "result[\"Predicted Rank\"] = result[\"prediction\"].map(class_mapping)\n",
    "result[\"Probability\"] = result[\"probability\"].apply(\n",
    "    lambda x: f\"Bottom 50%: {x[0]:.2%}, Top 50%: {x[1]:.2%}, Top 25%: {x[2]:.2%}, Top 10%: {x[3]:.2%}\"\n",
    ")\n",
    "\n",
    "# 최종 출력\n",
    "print(result[[\"Predicted Rank\", \"Probability\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "deeb920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 입력 데이터\n",
    "input_data = [[1.0, 33.0, 1608.0, 5975.0, 3091.0]]  # Gender, Age, Swim, Bike, Run in seconds\n",
    "\n",
    "# 입력 데이터를 Spark DataFrame으로 생성\n",
    "input_df = spark.createDataFrame(input_data, schema=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"])\n",
    "\n",
    "# 피처 벡터 생성\n",
    "input_features = assembler.transform(input_df)\n",
    "\n",
    "# 예측 수행\n",
    "predictions = rf_model.transform(input_features)\n",
    "\n",
    "# 결과 Pandas로 변환\n",
    "result_pd = predictions.select(\"prediction\", \"probability\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "094674b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 매핑\n",
    "class_mapping = {\n",
    "    0.0: \"Bottom 50%\",\n",
    "    1.0: \"Top 50%\",\n",
    "    2.0: \"Top 25%\",\n",
    "    3.0: \"Top 10%\"\n",
    "}\n",
    "\n",
    "# 예상 종합 순위 그룹\n",
    "overall_rank = class_mapping[result_pd[\"prediction\"].iloc[0]]\n",
    "\n",
    "# 부문별 순위 그룹 (예제: M30-34 그룹)\n",
    "division_rank = \"Top 10%\"  # 부문 순위는 추가 데이터가 필요하거나 가정할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c61da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 10% 기준 (예시 데이터)\n",
    "top_10_criteria = {\n",
    "    \"swim_seconds\": 3900,  # 1시간 5분\n",
    "    \"bike_seconds\": 21600, # 6시간\n",
    "    \"run_seconds\": 16200   # 4시간 30분\n",
    "}\n",
    "\n",
    "# 개선 방향 계산\n",
    "improvement_suggestions = []\n",
    "\n",
    "if input_data[0][2] > top_10_criteria[\"swim_seconds\"]:\n",
    "    improvement_suggestions.append(f\"수영: 상위 10%에 진입하려면 {top_10_criteria['swim_seconds']//3600}시간 {top_10_criteria['swim_seconds']%3600//60}분 이하로 줄여야 합니다.\")\n",
    "if input_data[0][3] > top_10_criteria[\"bike_seconds\"]:\n",
    "    improvement_suggestions.append(f\"자전거: 상위 10%에 진입하려면 {top_10_criteria['bike_seconds']//3600}시간 {top_10_criteria['bike_seconds']%3600//60}분 이하로 줄여야 합니다.\")\n",
    "if input_data[0][4] > top_10_criteria[\"run_seconds\"]:\n",
    "    improvement_suggestions.append(f\"달리기: 상위 10%에 진입하려면 {top_10_criteria['run_seconds']//3600}시간 {top_10_criteria['run_seconds']%3600//60}분 이하로 줄여야 합니다.\")\n",
    "else:\n",
    "    improvement_suggestions.append(\"달리기: 현재 기록이 평균 이상이며 추가 개선은 필요하지 않습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e762cd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터:\n",
      "- 성별: 남성\n",
      "- 나이: 33세\n",
      "- 기록:\n",
      "    - 수영 (3.8km): 01:10:00 (1시간 10분)\n",
      "    - 자전거 (180km): 06:20:00 (6시간 20분)\n",
      "    - 달리기 (42.2km): 04:30:00 (4시간 30분)\n",
      "\n",
      "모델 예측 결과:\n",
      "- 예상 종합 순위 그룹: Top 10%\n",
      "- 예상 부문 순위 그룹 (M30-34): Top 10%\n",
      "\n",
      "종목별 개선 방향:\n",
      "    - 달리기: 현재 기록이 평균 이상이며 추가 개선은 필요하지 않습니다.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3d8e85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852014bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
