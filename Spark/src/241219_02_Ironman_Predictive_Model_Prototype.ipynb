{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce332bff",
   "metadata": {},
   "source": [
    "1단계: 데이터 로딩 및 초기 확인\n",
    "주요 작업\n",
    "PySpark DataFrame 생성\n",
    "CSV 파일을 PySpark의 read.csv() 메서드를 이용해 읽어옵니다.\n",
    "\n",
    "header=True: 첫 번째 줄을 헤더로 사용합니다.\n",
    "inferSchema=True: 컬럼 데이터 타입을 자동으로 추론합니다.\n",
    "데이터 확인\n",
    "\n",
    "df.show(truncate=False): 데이터 샘플 확인.\n",
    "df.printSchema(): 데이터의 구조(컬럼 이름, 타입 등)를 확인.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e9e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/20 11:48:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------+------+----+--------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "|bib|name                |country       |gender|div |div_rank|overall_time|overall_rank|swim_time|swim_rank|bike_time|bike_rank|run_time|run_rank|finish_status|\n",
      "+---+--------------------+--------------+------+----+--------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "|8  |Gustav Iden         |Norway        |Male  |MPRO|1       |7:40:24     |1           |48:23:00 |10       |4:11:06  |6        |2:36:15 |1       |Finisher     |\n",
      "|15 |Sam Laidlow         |France        |Male  |MPRO|2       |7:42:24     |2           |48:16:00 |2        |4:04:36  |1        |2:44:40 |5       |Finisher     |\n",
      "|1  |Kristian Blummenfelt|Norway        |Male  |MPRO|3       |7:43:23     |3           |48:20:00 |5        |4:11:16  |8        |2:39:21 |2       |Finisher     |\n",
      "|23 |Max Neumann         |Australia     |Male  |MPRO|4       |7:44:44     |4           |48:25:00 |13       |4:11:30  |9        |2:40:14 |3       |Finisher     |\n",
      "|17 |Joe Skipper         |United Kingdom|Male  |MPRO|5       |7:54:05     |5           |52:55:00 |60       |4:11:11  |7        |2:45:26 |6       |Finisher     |\n",
      "|7  |Sebastian Kienle    |Germany       |Male  |MPRO|6       |7:55:40     |6           |52:58:00 |66       |4:09:11  |4        |2:48:45 |13      |Finisher     |\n",
      "|12 |Leon Chevalier      |France        |Male  |MPRO|7       |7:55:52     |7           |52:54:00 |59       |4:09:05  |3        |2:49:28 |15      |Finisher     |\n",
      "|32 |Magnus Ditlev       |Denmark       |Male  |MPRO|8       |7:56:38     |8           |49:49:00 |32       |4:13:38  |11       |2:48:11 |11      |Finisher     |\n",
      "|38 |Clement Mignon      |France        |Male  |MPRO|9       |7:56:58     |9           |49:50:00 |33       |4:15:14  |14       |2:46:00 |8       |Finisher     |\n",
      "|6  |Patrick Lange       |Germany       |Male  |MPRO|10      |7:58:20     |10          |49:42:00 |26       |4:21:52  |22       |2:41:59 |4       |Finisher     |\n",
      "|11 |Cameron Wurf        |Australia     |Male  |MPRO|11      |8:00:51     |11          |52:51:00 |56       |4:09:04  |2        |2:54:27 |19      |Finisher     |\n",
      "|5  |Florian Angert      |Germany       |Male  |MPRO|12      |8:01:53     |12          |48:15:00 |1        |4:17:58  |19       |2:50:29 |16      |Finisher     |\n",
      "|9  |Timothy O'Donnell   |United States |Male  |MPRO|13      |8:02:58     |13          |48:23:00 |11       |4:13:30  |10       |2:56:03 |21      |Finisher     |\n",
      "|21 |Denis Chevrot       |France        |Male  |MPRO|14      |8:03:24     |14          |48:26:00 |16       |4:22:59  |25       |2:47:03 |9       |Finisher     |\n",
      "|20 |Matthew Hanson      |United States |Male  |MPRO|15      |8:04:55     |15          |52:40:00 |51       |4:22:18  |24       |2:45:34 |7       |Finisher     |\n",
      "|44 |Mathias Petersen    |Denmark       |Male  |MPRO|16      |8:06:45     |16          |48:25:00 |14       |4:24:55  |31       |2:48:16 |12      |Finisher     |\n",
      "|33 |Bradley Weiss       |South Africa  |Male  |MPRO|17      |8:07:28     |17          |49:41:00 |25       |4:24:49  |30       |2:48:01 |10      |Finisher     |\n",
      "|47 |Luciano Taccone     |Argentina     |Male  |MPRO|18      |8:09:10     |18          |49:47:00 |30       |4:25:08  |33       |2:49:18 |14      |Finisher     |\n",
      "|52 |Henrik Goesch       |Finland       |Male  |MPRO|19      |8:10:25     |19          |49:47:00 |31       |4:21:57  |23       |2:53:49 |18      |Finisher     |\n",
      "|19 |Rudy Von Berg       |United States |Male  |MPRO|20      |8:12:47     |20          |49:43:00 |28       |4:15:24  |15       |3:02:17 |34      |Finisher     |\n",
      "+---+--------------------+--------------+------+----+--------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- bib: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- div: string (nullable = true)\n",
      " |-- div_rank: integer (nullable = true)\n",
      " |-- overall_time: string (nullable = true)\n",
      " |-- overall_rank: integer (nullable = true)\n",
      " |-- swim_time: string (nullable = true)\n",
      " |-- swim_rank: integer (nullable = true)\n",
      " |-- bike_time: string (nullable = true)\n",
      " |-- bike_rank: integer (nullable = true)\n",
      " |-- run_time: string (nullable = true)\n",
      " |-- run_rank: integer (nullable = true)\n",
      " |-- finish_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark 세션 초기화\n",
    "spark = SparkSession.builder.appName(\"Ironman Data Analysis_241219_02\").getOrCreate()\n",
    "\n",
    "# CSV 다시 불러오기\n",
    "file_path = \"file:///home/lab12/src/data/ironman_wc_2022.csv\"  # 정확한 경로 입력\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# 데이터 확인\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10ac63",
   "metadata": {},
   "source": [
    "2단계: 시간 데이터 처리\n",
    "\n",
    "대회 기록 데이터를 문자열 형식(hh:mm:ss)에서 초 단위 데이터로 변환합니다.\n",
    "변환된 데이터를 새로운 컬럼으로 추가해, 이후 분석과 모델 학습에 활용할 준비를 합니다.\n",
    "\n",
    "시간 데이터를 초 단위로 변환\n",
    "\n",
    "time_to_seconds 함수는 문자열 시간 데이터를 분리하여 초 단위로 계산합니다.\n",
    "split(time_str_col, \":\"): hh:mm:ss 문자열을 : 기준으로 나눕니다.\n",
    "[0], [1], [2]: 각각 시, 분, 초를 나타냅니다.\n",
    ".cast(\"int\"): 문자열 값을 정수로 변환.\n",
    "\n",
    "새로운 컬럼 추가\n",
    "\n",
    "withColumn()을 사용하여 기존 시간 데이터를 변환한 결과를 새로운 컬럼으로 추가.\n",
    "swim_seconds: 수영 기록(초 단위).\n",
    "bike_seconds: 자전거 기록(초 단위).\n",
    "run_seconds: 달리기 기록(초 단위).\n",
    "overall_seconds: 총합 기록(초 단위).\n",
    "\n",
    "\n",
    "컷오프 기준 설정\n",
    "\n",
    "수영 컷오프: 수영 기록이 2시간 20분(8400초)을 초과하면 DNF.\n",
    "수영 + 자전거 컷오프: 수영 + 자전거 기록 합이 10시간 30분(37800초)을 초과하면 DNF.\n",
    "총합 컷오프: 전체 기록이 17시간(61200초)을 초과하면 DNF.\n",
    "\n",
    "조건문 사용 (when)\n",
    "\n",
    "when 함수는 조건을 만족하면 1(DNF)을, 그렇지 않으면 0(완주)을 반환합니다.\n",
    "여러 조건을 |(OR)로 연결하여, 한 가지라도 초과하면 1로 표시.\n",
    "새로운 컬럼 추가 (withColumn)\n",
    "\n",
    "DNF 컬럼은 각 참가자의 컷오프 상태를 나타냅니다:\n",
    "1: 컷오프 초과 (DNF).\n",
    "0: 컷오프 이내 (완주)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "639334e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+---------------+---+\n",
      "|swim_seconds|bike_seconds|run_seconds|overall_seconds|DNF|\n",
      "+------------+------------+-----------+---------------+---+\n",
      "|      174180|       15066|       9375|          27624|  1|\n",
      "|      173760|       14676|       9880|          27744|  1|\n",
      "|      174000|       15076|       9561|          27803|  1|\n",
      "|      174300|       15090|       9614|          27884|  1|\n",
      "|      190500|       15071|       9926|          28445|  1|\n",
      "|      190680|       14951|      10125|          28540|  1|\n",
      "|      190440|       14945|      10168|          28552|  1|\n",
      "|      179340|       15218|      10091|          28598|  1|\n",
      "|      179400|       15314|       9960|          28618|  1|\n",
      "|      178920|       15712|       9719|          28700|  1|\n",
      "|      190260|       14944|      10467|          28851|  1|\n",
      "|      173700|       15478|      10229|          28913|  1|\n",
      "|      174180|       15210|      10563|          28978|  1|\n",
      "|      174360|       15779|      10023|          29004|  1|\n",
      "|      189600|       15738|       9934|          29095|  1|\n",
      "|      174300|       15895|      10096|          29205|  1|\n",
      "|      178860|       15889|      10081|          29248|  1|\n",
      "|      179220|       15908|      10158|          29350|  1|\n",
      "|      179220|       15717|      10429|          29425|  1|\n",
      "|      178980|       15324|      10937|          29567|  1|\n",
      "+------------+------------+-----------+---------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col, split\n",
    "\n",
    "# 1. 시간 데이터를 초 단위로 변환하는 함수 정의\n",
    "def time_to_seconds(time_str_col):\n",
    "    \"\"\"\n",
    "    문자열 형태의 시간 데이터를 초 단위로 변환\n",
    "    \"\"\"\n",
    "    return (split(time_str_col, \":\")[0].cast(\"int\") * 3600 +\n",
    "            split(time_str_col, \":\")[1].cast(\"int\") * 60 +\n",
    "            split(time_str_col, \":\")[2].cast(\"int\"))\n",
    "\n",
    "# 2. 초 단위 컬럼 생성\n",
    "df = df.withColumn(\"swim_seconds\", time_to_seconds(col(\"swim_time\"))) \\\n",
    "       .withColumn(\"bike_seconds\", time_to_seconds(col(\"bike_time\"))) \\\n",
    "       .withColumn(\"run_seconds\", time_to_seconds(col(\"run_time\"))) \\\n",
    "       .withColumn(\"overall_seconds\", time_to_seconds(col(\"overall_time\")))\n",
    "\n",
    "# 3. 컷오프 기준에 따른 DNF 컬럼 생성\n",
    "df = df.withColumn(\n",
    "    \"DNF\",\n",
    "    when((col(\"swim_seconds\") > 2 * 3600 + 20 * 60) |  # 수영 컷오프\n",
    "         (col(\"swim_seconds\") + col(\"bike_seconds\") > 10 * 3600 + 30 * 60) |  # 수영 + 사이클 컷오프\n",
    "         (col(\"overall_seconds\") > 17 * 3600), 1).otherwise(0)  # 전체 컷오프\n",
    ")\n",
    "\n",
    "# 4. 결과 확인\n",
    "df.select(\"swim_seconds\", \"bike_seconds\", \"run_seconds\", \"overall_seconds\", \"DNF\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac63b4b1",
   "metadata": {},
   "source": [
    "DNS/DQ 상태 제거\n",
    "\n",
    "DNS: \"Did Not Start\" (참가하지 않음).\n",
    "DQ: \"Disqualified\" (실격).\n",
    "이 두 상태는 대회 규칙상 기록이 없거나 무효 처리된 데이터이므로 제거합니다.\n",
    "결측값 제거\n",
    "\n",
    "dropna()를 사용해 특정 컬럼(swim_seconds, bike_seconds, run_seconds, overall_seconds)에 결측값이 있으면 해당 행을 제거합니다.\n",
    "모든 기록이 없는 데이터는 분석과 모델 학습에 방해가 되기 때문입니다.\n",
    "데이터 타입 변환\n",
    "\n",
    "숫자 연산이 정확하게 이루어지도록 기록 데이터를 double(실수형)로 변환합니다.\n",
    "이는 이후 학습 모델에서 데이터 타입 불일치로 인한 오류를 방지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a3336c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter((col(\"finish_status\") != \"DNS\") & (col(\"finish_status\") != \"DQ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f80107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"swim_seconds\", \"bike_seconds\", \"run_seconds\", \"overall_seconds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0035e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"swim_seconds\", col(\"swim_seconds\").cast(\"double\")) \\\n",
    "       .withColumn(\"bike_seconds\", col(\"bike_seconds\").cast(\"double\")) \\\n",
    "       .withColumn(\"run_seconds\", col(\"run_seconds\").cast(\"double\")) \\\n",
    "       .withColumn(\"overall_seconds\", col(\"overall_seconds\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32747c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|DNF|count|\n",
      "+---+-----+\n",
      "|  1|  345|\n",
      "|  0| 2031|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"DNF\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b73e3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|DNF|count|\n",
      "+---+-----+\n",
      "|  0| 2031|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_finishers = df.filter(col(\"DNF\") == 0)\n",
    "df_finishers.select(\"DNF\").groupBy(\"DNF\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d01481e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|gender|gender_encoded|\n",
      "+------+--------------+\n",
      "|  Male|           0.0|\n",
      "|  Male|           0.0|\n",
      "|  Male|           0.0|\n",
      "|  Male|           0.0|\n",
      "|  Male|           0.0|\n",
      "+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# gender 컬럼 인코딩\n",
    "indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_encoded\")\n",
    "df_finishers = indexer.fit(df_finishers).transform(df_finishers)\n",
    "\n",
    "# 결과 확인\n",
    "df_finishers.select(\"gender\", \"gender_encoded\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5ab5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|   div|age_group|\n",
      "+------+---------+\n",
      "|M30-34|       30|\n",
      "|M25-29|       25|\n",
      "|M18-24|       18|\n",
      "|M35-39|       35|\n",
      "|M40-44|       40|\n",
      "|M55-59|       55|\n",
      "|M45-49|       45|\n",
      "|M50-54|       50|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# div 컬럼을 기반으로 age_group 생성\n",
    "df_finishers = df_finishers.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"div\").startswith(\"M18-24\"), 18)\n",
    "    .when(col(\"div\").startswith(\"M25-29\"), 25)\n",
    "    .when(col(\"div\").startswith(\"M30-34\"), 30)\n",
    "    .when(col(\"div\").startswith(\"M35-39\"), 35)\n",
    "    .when(col(\"div\").startswith(\"M40-44\"), 40)\n",
    "    .when(col(\"div\").startswith(\"M45-49\"), 45)\n",
    "    .when(col(\"div\").startswith(\"M50-54\"), 50)\n",
    "    .when(col(\"div\").startswith(\"M55-59\"), 55)\n",
    "    .when(col(\"div\").startswith(\"MPRO\"), 0)  # 프로 선수\n",
    "    .otherwise(None)  # 나머지 경우 처리\n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "df_finishers.select(\"div\", \"age_group\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778689fb",
   "metadata": {},
   "source": [
    "전체 참가자 수 계산\n",
    "\n",
    "df.count(): 데이터셋에 포함된 전체 참가자 수를 계산합니다.\n",
    "이를 바탕으로 각 그룹의 기준(Top 10%, Top 25%, 등)을 설정합니다.\n",
    "순위 그룹 기준 설정\n",
    "\n",
    "Top 10%: 전체 참가자 중 상위 10%.\n",
    "Top 25%: 상위 10% 이후 ~ 상위 25% 이내.\n",
    "Top 50%: 상위 25% 이후 ~ 상위 50% 이내.\n",
    "Bottom 50%: 하위 50%.\n",
    "rank_range 컬럼 생성\n",
    "\n",
    "when 조건문을 사용해 각 참가자의 overall_rank를 기준으로 rank_range 컬럼에 그룹 값을 할당합니다:\n",
    "예: overall_rank가 100명 중 5위라면 Top 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68dcac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----------+\n",
      "|features                         |rank_range|\n",
      "+---------------------------------+----------+\n",
      "|[0.0,30.0,3655.0,16953.0,11146.0]|Top 10%   |\n",
      "|[0.0,30.0,3983.0,17318.0,10560.0]|Top 10%   |\n",
      "|[0.0,30.0,3755.0,17022.0,11094.0]|Top 10%   |\n",
      "|[0.0,35.0,4042.0,16196.0,11461.0]|Top 10%   |\n",
      "|[0.0,30.0,3955.0,17532.0,10395.0]|Top 10%   |\n",
      "+---------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# 1. 순위 그룹 라벨링\n",
    "total_participants = df_finishers.count()\n",
    "top_10 = total_participants * 0.1\n",
    "top_25 = total_participants * 0.25\n",
    "top_50 = total_participants * 0.5\n",
    "\n",
    "df_finishers = df_finishers.withColumn(\n",
    "    \"rank_range\",\n",
    "    when(col(\"overall_rank\") <= top_10, \"Top 10%\")\n",
    "    .when((col(\"overall_rank\") > top_10) & (col(\"overall_rank\") <= top_25), \"Top 25%\")\n",
    "    .when((col(\"overall_rank\") > top_25) & (col(\"overall_rank\") <= top_50), \"Top 50%\")\n",
    "    .otherwise(\"Bottom 50%\")\n",
    ")\n",
    "\n",
    "# 2. 피처 벡터 생성\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_final = assembler.transform(df_finishers).select(\"features\", \"rank_range\")\n",
    "\n",
    "# 결과 확인\n",
    "df_final.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a09e93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7611b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|rank_range|rank_range_index|\n",
      "+----------+----------------+\n",
      "|   Top 10%|             3.0|\n",
      "|   Top 25%|             2.0|\n",
      "|Bottom 50%|             0.0|\n",
      "|   Top 50%|             1.0|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rank_range를 숫자형 rank_range_index로 변환\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"rank_range\", outputCol=\"rank_range_index\")\n",
    "df_final = indexer.fit(df_final).transform(df_final)\n",
    "\n",
    "# 결과 확인\n",
    "df_final.select(\"rank_range\", \"rank_range_index\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09975e4f",
   "metadata": {},
   "source": [
    "randomSplit()을 사용해 데이터를 학습용(80%)과 테스트용(20%)으로 나눕니다:\n",
    "학습 데이터(train_data): 모델 학습에 사용.\n",
    "테스트 데이터(test_data): 학습된 모델의 성능 평가에 사용.\n",
    "seed: 데이터를 분할할 때 결과를 재현 가능하게 하는 고정값."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e996505e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 크기: 1671\n",
      "테스트 데이터 크기: 360\n",
      "+---------------------------------+----------+----------------+\n",
      "|features                         |rank_range|rank_range_index|\n",
      "+---------------------------------+----------+----------------+\n",
      "|[0.0,18.0,3653.0,20368.0,12874.0]|Top 50%   |1.0             |\n",
      "|[0.0,18.0,3693.0,19738.0,17137.0]|Bottom 50%|0.0             |\n",
      "|[0.0,18.0,3722.0,19532.0,17446.0]|Bottom 50%|0.0             |\n",
      "|[0.0,18.0,3729.0,18523.0,14229.0]|Top 50%   |1.0             |\n",
      "|[0.0,18.0,3732.0,18024.0,16481.0]|Bottom 50%|0.0             |\n",
      "+---------------------------------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------------------------+----------+----------------+\n",
      "|features                         |rank_range|rank_range_index|\n",
      "+---------------------------------+----------+----------------+\n",
      "|[0.0,18.0,3717.0,19283.0,13253.0]|Top 50%   |1.0             |\n",
      "|[0.0,18.0,3776.0,18633.0,12743.0]|Top 50%   |1.0             |\n",
      "|[0.0,18.0,3855.0,19772.0,14458.0]|Bottom 50%|0.0             |\n",
      "|[0.0,18.0,3943.0,18864.0,14091.0]|Top 50%   |1.0             |\n",
      "|[0.0,18.0,4134.0,18740.0,17225.0]|Bottom 50%|0.0             |\n",
      "+---------------------------------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할\n",
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(\"훈련 데이터 크기:\", train_data.count())\n",
    "print(\"테스트 데이터 크기:\", test_data.count())\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "train_data.show(5, truncate=False)\n",
    "test_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ae48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Forest 모델 생성\n",
    "\n",
    "RandomForestClassifier: 분류 모델로, 여러 개의 의사결정 트리를 앙상블 방식으로 사용.\n",
    "labelCol: 모델의 레이블(정답) 컬럼 (rank_range_index).\n",
    "featuresCol: 모델의 입력값(특성 벡터) 컬럼 (features).\n",
    "numTrees: 생성할 트리의 개수 (여기서는 50개).\n",
    "\n",
    "모델 학습\n",
    "\n",
    "rf.fit(train_data): 학습 데이터를 사용해 Random Forest 모델을 학습.\n",
    "\n",
    "테스트 데이터로 예측\n",
    "\n",
    "rf_model.transform(test_data): 학습된 모델로 테스트 데이터를 예측.\n",
    "prediction: 모델이 예측한 순위 그룹.\n",
    "probability: 각 순위 그룹에 속할 확률."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f28cf000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----------------+----------+----------------------------------------------------------------------------------+\n",
      "|features                         |rank_range_index|prediction|probability                                                                       |\n",
      "+---------------------------------+----------------+----------+----------------------------------------------------------------------------------+\n",
      "|[0.0,18.0,3717.0,19283.0,13253.0]|1.0             |1.0       |[0.15239751076467112,0.814753365129092,0.03277894866764049,7.017543859649125E-5]  |\n",
      "|[0.0,18.0,3776.0,18633.0,12743.0]|1.0             |1.0       |[0.05188127046916252,0.598255425232573,0.3097798372675162,0.0400834670307482]     |\n",
      "|[0.0,18.0,3855.0,19772.0,14458.0]|0.0             |0.0       |[0.941644413078116,0.058355586921883945,0.0,0.0]                                  |\n",
      "|[0.0,18.0,3943.0,18864.0,14091.0]|1.0             |1.0       |[0.407518233720109,0.5533031052250287,0.039019596727377065,1.5906432748538012E-4] |\n",
      "|[0.0,18.0,4134.0,18740.0,17225.0]|0.0             |0.0       |[0.9706079553175495,0.02801841830882409,0.0013736263736263735,0.0]                |\n",
      "|[0.0,18.0,4212.0,18675.0,13225.0]|1.0             |1.0       |[0.09294357325345642,0.8305740576046672,0.07632330481439104,1.5906432748538014E-4]|\n",
      "|[0.0,18.0,4340.0,17468.0,13672.0]|1.0             |1.0       |[0.06414278963329238,0.6195368180655614,0.2877471580688675,0.028573234232278807]  |\n",
      "|[0.0,18.0,4702.0,20051.0,13990.0]|0.0             |0.0       |[0.9680482630786773,0.03195173692132265,0.0,0.0]                                  |\n",
      "|[0.0,30.0,3600.0,19602.0,14738.0]|0.0             |0.0       |[0.9760962816790534,0.023903718320946617,0.0,0.0]                                 |\n",
      "|[0.0,30.0,3602.0,17998.0,13419.0]|1.0             |1.0       |[0.053597866135857215,0.6041140264658312,0.3165791955266282,0.025708911871683258] |\n",
      "+---------------------------------+----------------+----------+----------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Random Forest 모델 생성 및 학습\n",
    "rf = RandomForestClassifier(labelCol=\"rank_range_index\", featuresCol=\"features\", numTrees=50)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# 테스트 데이터로 예측\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# 결과 확인\n",
    "predictions.select(\"features\", \"rank_range_index\", \"prediction\", \"probability\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de51cd",
   "metadata": {},
   "source": [
    "예측 수행\n",
    "\n",
    "rf_model.transform(test_data)를 통해 테스트 데이터에서 예측을 수행합니다:\n",
    "prediction: 모델이 예측한 순위 그룹 (0: Bottom 50%, 1: Top 50%, 2: Top 25%, 3: Top 10%).\n",
    "probability: 각 순위 그룹에 속할 확률 분포.\n",
    "\n",
    "결과 출력\n",
    "\n",
    "select()로 관심 있는 컬럼을 선택해 결과를 확인합니다:\n",
    "features: 모델에 입력된 특성 벡터.\n",
    "rank_range_index: 실제 레이블(정답).\n",
    "prediction: 모델의 예측값.\n",
    "probability: 각 클래스에 속할 확률 벡터."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302f38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d313043",
   "metadata": {},
   "source": [
    "모델 정확도와 F1-스코어\n",
    "모델 정확도: 0.95\n",
    "전체 예측 중 95%가 정답이었다는 의미입니다.\n",
    "매우 높은 정확도를 보여주는 결과입니다.\n",
    "모델 F1-스코어: 0.95\n",
    "Precision(정밀도)과 Recall(재현율)을 조화롭게 고려한 지표로, 모델이 클래스 불균형 데이터에서도 우수한 성능을 보였음을 나타냅니다.\n",
    "정확도와 F1-스코어가 동일하거나 비슷하다면, 모델이 전반적으로 균형 잡힌 성능을 가지고 있음을 의미합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08a944f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 정확도: 0.95\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 평가 지표: 정확도\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"rank_range_index\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"모델 정확도: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e1fc7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 F1-스코어: 0.95\n"
     ]
    }
   ],
   "source": [
    "# 평가 지표: F1-스코어\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"rank_range_index\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "print(f\"모델 F1-스코어: {f1_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a297576",
   "metadata": {},
   "source": [
    "rank_range_index: 실제 정답 레이블.\n",
    "prediction: 모델이 예측한 값.\n",
    "count: 해당 레이블과 예측값의 조합에 속하는 데이터의 개수.\n",
    "예시 분석:\n",
    "\n",
    "1.0 → 1.0 (76)\n",
    "실제 순위 그룹이 Top 50%(1.0)인 참가자 76명을 정확히 예측했습니다.\n",
    "3.0 → 2.0 (4)\n",
    "실제 Top 10%(3.0)인 참가자 4명을 Top 25%(2.0)로 잘못 예측했습니다.\n",
    "0.0 → 0.0 (234)\n",
    "실제 Bottom 50%(0.0)인 참가자 234명을 정확히 예측했습니다.\n",
    "1.0 → 0.0 (4)\n",
    "실제 Top 50%(1.0)인 참가자 4명을 Bottom 50%(0.0)로 잘못 예측했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c93f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:===================================================>   (70 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-----+\n",
      "|rank_range_index|prediction|count|\n",
      "+----------------+----------+-----+\n",
      "|             1.0|       1.0|   76|\n",
      "|             3.0|       2.0|    4|\n",
      "|             0.0|       1.0|    4|\n",
      "|             1.0|       0.0|    4|\n",
      "|             2.0|       2.0|   26|\n",
      "|             2.0|       1.0|    3|\n",
      "|             1.0|       2.0|    2|\n",
      "|             0.0|       0.0|  234|\n",
      "|             3.0|       3.0|    7|\n",
      "+----------------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 예측 및 실제값 카운트\n",
    "predictions.groupBy(\"rank_range_index\", \"prediction\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d1c631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_encoded: 0.00\n",
      "age_group: 0.01\n",
      "swim_seconds: 0.07\n",
      "bike_seconds: 0.36\n",
      "run_seconds: 0.57\n"
     ]
    }
   ],
   "source": [
    "# 특성 중요도 확인\n",
    "feature_importance = rf_model.featureImportances\n",
    "features = [\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"]\n",
    "\n",
    "# 결과 출력\n",
    "for feature, importance in zip(features, feature_importance):\n",
    "    print(f\"{feature}: {importance:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5904e",
   "metadata": {},
   "source": [
    "사용자 입력 받기\n",
    "\n",
    "성별, 나이, 수영, 자전거, 달리기 기록을 사용자로부터 입력받습니다.\n",
    "입력받은 기록을 Ironman 대회의 거리 기준으로 변환합니다.\n",
    "컷오프 기준 확인\n",
    "\n",
    "수영 + 자전거 기록, 전체 기록을 컷오프 기준과 비교합니다.\n",
    "기준을 초과하면 완주 불가능 메시지를 출력합니다.\n",
    "모델 예측\n",
    "\n",
    "입력 데이터를 특성 벡터로 변환하고, 학습된 모델에 전달합니다.\n",
    "모델은 순위 그룹을 예측하고 확률 분포를 반환합니다.\n",
    "예상 기록과 개선 방향 제공\n",
    "\n",
    "모델 예측 결과를 기반으로 예상 순위 그룹을 출력합니다.\n",
    "예상 기록(수영, 자전거, 달리기)과 상위 10% 진입을 위한 목표 기록을 제시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08ae0eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자신의 정보를 입력해주세요:\n",
      "성별 (남성/여성): 남성\n",
      "나이: 33\n",
      "수영 기록 (hh:mm:ss, 1.5km 기준): 02:00:00\n",
      "자전거 기록 (hh:mm:ss, 40km 기준): 02:00:00\n",
      "달리기 기록 (hh:mm:ss, 10km 기준): 02:00:00\n",
      "\n",
      "완주 불가능: 입력된 기록이 대회 컷오프 기준을 초과했습니다.\n",
      "    - 수영 + 자전거: 14:04:00 (컷오프: 10:30:00)\n",
      "    - 전체 시간: 22:30:24 (컷오프: 17:00:00)\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 임포트\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# 시간 데이터를 초 단위로 변환하는 함수\n",
    "def time_to_seconds(time_str):\n",
    "    h, m, s = map(int, time_str.split(\":\"))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "# 초를 hh:mm:ss 형식으로 변환하는 함수\n",
    "def seconds_to_time(seconds):\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = int(seconds % 60)\n",
    "    return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "# 클래스 매핑\n",
    "class_mapping = {\n",
    "    0.0: \"Bottom 50%\",\n",
    "    1.0: \"Top 50%\",\n",
    "    2.0: \"Top 25%\",\n",
    "    3.0: \"Top 10%\"\n",
    "}\n",
    "\n",
    "# 예상 기록 계산 함수\n",
    "def calculate_average_times(predicted_group):\n",
    "    avg_times = {\n",
    "        \"Bottom 50%\": {\"swim\": 1.5 * 3600, \"bike\": 6.5 * 3600, \"run\": 4.5 * 3600},\n",
    "        \"Top 50%\": {\"swim\": 1.4 * 3600, \"bike\": 6.0 * 3600, \"run\": 4.0 * 3600},\n",
    "        \"Top 25%\": {\"swim\": 1.3 * 3600, \"bike\": 5.5 * 3600, \"run\": 3.5 * 3600},\n",
    "        \"Top 10%\": {\"swim\": 1.2 * 3600, \"bike\": 5.0 * 3600, \"run\": 3.0 * 3600},\n",
    "    }\n",
    "    return avg_times[predicted_group]\n",
    "\n",
    "# 사용자 입력 데이터 받기\n",
    "print(\"자신의 정보를 입력해주세요:\")\n",
    "gender = input(\"성별 (남성/여성): \")\n",
    "age = int(input(\"나이: \"))\n",
    "swim_time = input(\"수영 기록 (hh:mm:ss, 1.5km 기준): \")\n",
    "bike_time = input(\"자전거 기록 (hh:mm:ss, 40km 기준): \")\n",
    "run_time = input(\"달리기 기록 (hh:mm:ss, 10km 기준): \")\n",
    "\n",
    "# 성별 인코딩 및 시간 변환\n",
    "gender_encoded = 1.0 if gender == \"남성\" else 0.0\n",
    "swim_seconds = time_to_seconds(swim_time) * (3.8 / 1.5)  # 수영 3.8km로 변환\n",
    "bike_seconds = time_to_seconds(bike_time) * (180 / 40)  # 자전거 180km로 변환\n",
    "run_seconds = time_to_seconds(run_time) * (42.2 / 10)  # 달리기 42.2km로 변환\n",
    "\n",
    "# 컷오프 기준\n",
    "SWIM_BIKE_CUTOFF = 10 * 3600 + 30 * 60  # 수영 + 자전거: 10시간 30분\n",
    "TOTAL_CUTOFF = 17 * 3600  # 전체 기록: 17시간\n",
    "\n",
    "# 입력 데이터 검증\n",
    "total_swim_bike = swim_seconds + bike_seconds\n",
    "total_time = total_swim_bike + run_seconds\n",
    "\n",
    "if total_swim_bike > SWIM_BIKE_CUTOFF or total_time > TOTAL_CUTOFF:\n",
    "    print(\"\\n완주 불가능: 입력된 기록이 대회 컷오프 기준을 초과했습니다.\")\n",
    "    print(f\"    - 수영 + 자전거: {seconds_to_time(total_swim_bike)} (컷오프: {seconds_to_time(SWIM_BIKE_CUTOFF)})\")\n",
    "    print(f\"    - 전체 시간: {seconds_to_time(total_time)} (컷오프: {seconds_to_time(TOTAL_CUTOFF)})\")\n",
    "else:\n",
    "    # 입력 데이터 생성\n",
    "    input_data = [[gender_encoded, age, swim_seconds, bike_seconds, run_seconds]]\n",
    "    input_df = spark.createDataFrame(input_data, schema=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\"])\n",
    "    input_features = assembler.transform(input_df)\n",
    "\n",
    "    # 모델 예측\n",
    "    predictions = rf_model.transform(input_features)\n",
    "    prediction = predictions.select(\"prediction\", \"probability\").collect()[0]\n",
    "    predicted_rank = class_mapping[prediction[\"prediction\"]]\n",
    "    probabilities = prediction[\"probability\"]\n",
    "\n",
    "    # 예상 기록 계산\n",
    "    average_times = calculate_average_times(predicted_rank)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"\\n모델 예측 결과:\")\n",
    "    print(f\"- 예상 종합 순위 그룹: {predicted_rank}\")\n",
    "    #print(f\"- 예상 부문 순위 그룹 (성별: {gender}, 나이 그룹: {age // 10 * 10}대): {predicted_rank}\")\n",
    "    # print(f\"- 예상 부문 순위 그룹 (성별: {gender}, 나이 그룹: {age // 10 * 10}대): {predicted_rank}\")\n",
    "    # 위 주석 처리된 출력은 모델 설계상 잘못된 정보일 수 있으므로 비활성화\n",
    "\n",
    "    print(\"\\n예상 기록:\")\n",
    "    print(f\"    - 수영: {seconds_to_time(average_times['swim'])}\")\n",
    "    print(f\"    - 자전거: {seconds_to_time(average_times['bike'])}\")\n",
    "    print(f\"    - 달리기: {seconds_to_time(average_times['run'])}\")\n",
    "\n",
    "    print(\"\\n종목별 개선 방향:\")\n",
    "    print(\"    - 수영(3.8km): 상위 10%에 진입하려면 1시간 3분 58초 이하로 줄여야 합니다.\")\n",
    "    print(\"    - 자전거(180km): 상위 10%에 진입하려면 4시간 48분 07초 이하로 줄여야 합니다.\")\n",
    "    print(\"    - 달리기(42.2km): 상위 10%에 진입하려면 3시간 15분 25초 이하로 줄여야 합니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f5f0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fabdf5a2",
   "metadata": {},
   "source": [
    "------- 각 부문별 평균 기록을 계산해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "835b31d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rank_range: string (nullable = false)\n",
      " |-- rank_range_index: double (nullable = false)\n",
      "\n",
      "+---------------------------------+----------+----------------+\n",
      "|features                         |rank_range|rank_range_index|\n",
      "+---------------------------------+----------+----------------+\n",
      "|[0.0,30.0,3655.0,16953.0,11146.0]|Top 10%   |3.0             |\n",
      "|[0.0,30.0,3983.0,17318.0,10560.0]|Top 10%   |3.0             |\n",
      "|[0.0,30.0,3755.0,17022.0,11094.0]|Top 10%   |3.0             |\n",
      "|[0.0,35.0,4042.0,16196.0,11461.0]|Top 10%   |3.0             |\n",
      "|[0.0,30.0,3955.0,17532.0,10395.0]|Top 10%   |3.0             |\n",
      "+---------------------------------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임 스키마 확인\n",
    "df_final.printSchema()\n",
    "\n",
    "# 컬럼 샘플 확인\n",
    "df_final.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40482e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----------+----------------+-------------------------------------+--------------+---------+------------+------------+-----------+\n",
      "|features                         |rank_range|rank_range_index|feature_array                        |gender_encoded|age_group|swim_seconds|bike_seconds|run_seconds|\n",
      "+---------------------------------+----------+----------------+-------------------------------------+--------------+---------+------------+------------+-----------+\n",
      "|[0.0,30.0,3655.0,16953.0,11146.0]|Top 10%   |3.0             |[0.0, 30.0, 3655.0, 16953.0, 11146.0]|0.0           |30.0     |3655.0      |16953.0     |11146.0    |\n",
      "|[0.0,30.0,3983.0,17318.0,10560.0]|Top 10%   |3.0             |[0.0, 30.0, 3983.0, 17318.0, 10560.0]|0.0           |30.0     |3983.0      |17318.0     |10560.0    |\n",
      "|[0.0,30.0,3755.0,17022.0,11094.0]|Top 10%   |3.0             |[0.0, 30.0, 3755.0, 17022.0, 11094.0]|0.0           |30.0     |3755.0      |17022.0     |11094.0    |\n",
      "|[0.0,35.0,4042.0,16196.0,11461.0]|Top 10%   |3.0             |[0.0, 35.0, 4042.0, 16196.0, 11461.0]|0.0           |35.0     |4042.0      |16196.0     |11461.0    |\n",
      "|[0.0,30.0,3955.0,17532.0,10395.0]|Top 10%   |3.0             |[0.0, 30.0, 3955.0, 17532.0, 10395.0]|0.0           |30.0     |3955.0      |17532.0     |10395.0    |\n",
      "+---------------------------------+----------+----------------+-------------------------------------+--------------+---------+------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# 벡터 데이터를 배열로 변환\n",
    "df_final = df_final.withColumn(\"feature_array\", vector_to_array(\"features\"))\n",
    "\n",
    "# 배열에서 각 요소를 개별 컬럼으로 분리\n",
    "df_final = df_final.withColumn(\"gender_encoded\", col(\"feature_array\")[0]) \\\n",
    "                   .withColumn(\"age_group\", col(\"feature_array\")[1]) \\\n",
    "                   .withColumn(\"swim_seconds\", col(\"feature_array\")[2]) \\\n",
    "                   .withColumn(\"bike_seconds\", col(\"feature_array\")[3]) \\\n",
    "                   .withColumn(\"run_seconds\", col(\"feature_array\")[4])\n",
    "\n",
    "# 결과 확인\n",
    "df_final.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "689cdabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10% 그룹 평균 기록:\n",
      " - 수영 (3.8km): 01:03:58\n",
      " - 자전거 (180km): 04:48:07\n",
      " - 달리기 (42.2km): 03:15:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 129:=================================>                   (126 + 2) / 200]\r",
      "\r",
      "[Stage 129:==============================================>      (176 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순위 그룹별 평균 기록:\n",
      "\n",
      "Bottom 50% 그룹:\n",
      " - 수영 (3.8km): 01:16:55\n",
      " - 자전거 (180km): 05:46:42\n",
      " - 달리기 (42.2km): 04:41:09\n",
      "\n",
      "Top 50% 그룹:\n",
      " - 수영 (3.8km): 01:08:23\n",
      " - 자전거 (180km): 05:07:53\n",
      " - 달리기 (42.2km): 03:44:57\n",
      "\n",
      "Top 10% 그룹:\n",
      " - 수영 (3.8km): 01:03:58\n",
      " - 자전거 (180km): 04:48:07\n",
      " - 달리기 (42.2km): 03:15:25\n",
      "\n",
      "Top 25% 그룹:\n",
      " - 수영 (3.8km): 01:06:09\n",
      " - 자전거 (180km): 04:56:43\n",
      " - 달리기 (42.2km): 03:29:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Top 10% 그룹 데이터 추출\n",
    "top10_group = df_final.filter(df_final[\"rank_range_index\"] == 3.0)\n",
    "\n",
    "# Top 10% 그룹의 평균 기록 계산\n",
    "top10_avg = top10_group.agg(\n",
    "    avg(\"swim_seconds\").alias(\"avg_swim_seconds\"),\n",
    "    avg(\"bike_seconds\").alias(\"avg_bike_seconds\"),\n",
    "    avg(\"run_seconds\").alias(\"avg_run_seconds\")\n",
    ").toPandas()\n",
    "\n",
    "# 초를 hh:mm:ss 형식으로 변환\n",
    "def seconds_to_time(seconds):\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = int(seconds % 60)\n",
    "    return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "# 결과 변환\n",
    "top10_swim = seconds_to_time(top10_avg[\"avg_swim_seconds\"][0])\n",
    "top10_bike = seconds_to_time(top10_avg[\"avg_bike_seconds\"][0])\n",
    "top10_run = seconds_to_time(top10_avg[\"avg_run_seconds\"][0])\n",
    "\n",
    "print(f\"Top 10% 그룹 평균 기록:\")\n",
    "print(f\" - 수영 (3.8km): {top10_swim}\")\n",
    "print(f\" - 자전거 (180km): {top10_bike}\")\n",
    "print(f\" - 달리기 (42.2km): {top10_run}\")\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# 벡터 데이터를 배열로 변환\n",
    "df_final = df_final.withColumn(\"feature_array\", vector_to_array(\"features\"))\n",
    "\n",
    "# 배열에서 각 요소를 개별 컬럼으로 분리\n",
    "df_final = df_final.withColumn(\"gender_encoded\", col(\"feature_array\")[0]) \\\n",
    "                   .withColumn(\"age_group\", col(\"feature_array\")[1]) \\\n",
    "                   .withColumn(\"swim_seconds\", col(\"feature_array\")[2]) \\\n",
    "                   .withColumn(\"bike_seconds\", col(\"feature_array\")[3]) \\\n",
    "                   .withColumn(\"run_seconds\", col(\"feature_array\")[4])\n",
    "\n",
    "# 그룹별 평균값 계산\n",
    "group_avg = df_final.groupBy(\"rank_range_index\").agg(\n",
    "    avg(\"swim_seconds\").alias(\"avg_swim_seconds\"),\n",
    "    avg(\"bike_seconds\").alias(\"avg_bike_seconds\"),\n",
    "    avg(\"run_seconds\").alias(\"avg_run_seconds\")\n",
    ").toPandas()\n",
    "\n",
    "# 초를 hh:mm:ss 형식으로 변환하는 함수\n",
    "def seconds_to_time(seconds):\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = int(seconds % 60)\n",
    "    return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "# 결과 변환 및 출력\n",
    "print(\"순위 그룹별 평균 기록:\")\n",
    "rank_mapping = {3.0: \"Top 10%\", 2.0: \"Top 25%\", 1.0: \"Top 50%\", 0.0: \"Bottom 50%\"}\n",
    "for _, row in group_avg.iterrows():\n",
    "    rank = rank_mapping[row[\"rank_range_index\"]]\n",
    "    swim_time = seconds_to_time(row[\"avg_swim_seconds\"])\n",
    "    bike_time = seconds_to_time(row[\"avg_bike_seconds\"])\n",
    "    run_time = seconds_to_time(row[\"avg_run_seconds\"])\n",
    "    print(f\"\\n{rank} 그룹:\")\n",
    "    print(f\" - 수영 (3.8km): {swim_time}\")\n",
    "    print(f\" - 자전거 (180km): {bike_time}\")\n",
    "    print(f\" - 달리기 (42.2km): {run_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfbb015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48454d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56228ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38a076b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/20 14:43:18 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-04002838-ebf5-45ab-8efc-8bca5d12858a/pyspark-915e155c-3656-4ee5-96ce-3e05ad389aae. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-04002838-ebf5-45ab-8efc-8bca5d12858a/pyspark-915e155c-3656-4ee5-96ce-3e05ad389aae\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1141)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f6243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
