{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3fc0360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 15:01:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-------------+------+--------+-------------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "|Bib|Name             |Country      |Gender|Division|Division Rank|Overall Time|Overall Rank|Swim Time|Swim Rank|Bike Time|Bike Rank|Run Time|Run Rank|Finish Status|\n",
      "+---+-----------------+-------------+------+--------+-------------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "|284|Sergei Khazov    |Kazakhstan   |Male  |M40-44  |1            |8:43:25     |1           |56:20:00 |3        |4:32:10  |3        |3:09:03 |3       |Finisher     |\n",
      "|6  |Sanghwan Oh      |Korea        |Male  |M50-54  |1            |8:52:37     |2           |1:15:00  |54       |4:20:06  |2        |3:11:19 |4       |Finisher     |\n",
      "|11 |Brenteson John   |United States|Male  |M30-34  |1            |9:06:06     |3           |1:00:20  |6        |4:43:08  |6        |3:17:23 |6       |Finisher     |\n",
      "|42 |Evgeniy Perevalov|Unknown      |Male  |M35-39  |1            |9:22:41     |4           |1:05:53  |14       |4:53:01  |12       |3:18:08 |7       |Finisher     |\n",
      "|102|Zijie Chen       |China        |Male  |M25-29  |1            |9:23:18     |5           |1:08:20  |23       |4:42:58  |5        |3:26:00 |11      |Finisher     |\n",
      "|17 |Hyungeun Kang    |Korea        |Male  |M35-39  |2            |9:25:22     |6           |1:05:14  |11       |4:47:01  |7        |3:25:50 |10      |Finisher     |\n",
      "|126|Diana Trubkovich |Uzbekistan   |Female|F25-29  |1            |9:30:46     |7           |56:23:00 |4        |5:08:37  |30       |3:19:59 |8       |Finisher     |\n",
      "|41 |Tsuyoshi Ohno    |Japan        |Male  |M35-39  |3            |9:35:24     |8           |1:07:15  |22       |4:51:37  |10       |3:28:48 |13      |Finisher     |\n",
      "|4  |Yong Jin Kim     |Korea        |Male  |M40-44  |2            |9:41:04     |9           |1:18:05  |90       |4:41:30  |4        |3:33:06 |17      |Finisher     |\n",
      "|334|박 진홍          |Korea        |Male  |M40-44  |3            |9:46:57     |10          |1:10:16  |25       |4:47:52  |8        |3:34:44 |19      |Finisher     |\n",
      "|43 |Xinwei Qian      |China        |Male  |M35-39  |4            |9:49:42     |11          |1:16:08  |62       |5:10:11  |33       |3:16:13 |5       |Finisher     |\n",
      "|22 |Hyeonsu Lee      |Korea        |Male  |M40-44  |4            |9:50:11     |12          |1:10:26  |26       |5:06:43  |26       |3:29:21 |14      |Finisher     |\n",
      "|200|Dong Kyu Kim     |Korea        |Male  |M35-39  |5            |9:53:54     |13          |1:15:40  |57       |4:53:58  |13       |3:37:52 |28      |Finisher     |\n",
      "|229|鹏飞 王          |China        |Male  |M35-39  |6            |10:03:25    |14          |1:21:19  |130      |5:03:55  |21       |3:31:15 |15      |Finisher     |\n",
      "|3  |Ling Er Choo     |Singapore    |Female|F35-39  |1            |10:05:44    |15          |1:10:50  |30       |5:12:33  |39       |3:36:12 |25      |Finisher     |\n",
      "|212|Seung Hwan Lee   |Korea        |Male  |M35-39  |7            |10:08:26    |16          |1:10:11  |24       |5:14:20  |49       |3:35:55 |23      |Finisher     |\n",
      "|55 |Kwangwon Kim     |Korea        |Male  |M45-49  |1            |10:12:39    |17          |1:06:51  |19       |4:52:45  |11       |4:04:33 |81      |Finisher     |\n",
      "|245|창연 이          |Korea        |Male  |M35-39  |8            |10:13:33    |18          |53:15:00 |1        |5:12:26  |37       |3:56:03 |57      |Finisher     |\n",
      "|61 |Hyungwon Moon    |Korea        |Male  |M50-54  |2            |10:17:20    |19          |1:21:01  |125      |5:26:50  |77       |3:22:14 |9       |Finisher     |\n",
      "|405|Yong Hyo Lee     |Korea        |Male  |M45-49  |2            |10:25:32    |20          |1:14:10  |47       |5:17:47  |55       |3:45:15 |36      |Finisher     |\n",
      "+---+-----------------+-------------+------+--------+-------------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Bib: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Division: string (nullable = true)\n",
      " |-- Division Rank: integer (nullable = true)\n",
      " |-- Overall Time: string (nullable = true)\n",
      " |-- Overall Rank: integer (nullable = true)\n",
      " |-- Swim Time: string (nullable = true)\n",
      " |-- Swim Rank: integer (nullable = true)\n",
      " |-- Bike Time: string (nullable = true)\n",
      " |-- Bike Rank: integer (nullable = true)\n",
      " |-- Run Time: string (nullable = true)\n",
      " |-- Run Rank: integer (nullable = true)\n",
      " |-- Finish Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark 세션 초기화\n",
    "spark = SparkSession.builder.appName(\"Ironman Data Analysis_02\").getOrCreate()\n",
    "\n",
    "# CSV 다시 불러오기\n",
    "file_path = \"file:///home/lab12/src/data/2024_ironman.csv\"  # 정확한 경로 입력\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# 데이터 확인\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2d9b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# 시간 문자열을 초 단위로 변환하는 함수\n",
    "def time_to_seconds(time_str):\n",
    "    if time_str:\n",
    "        h, m, s = map(int, time_str.split(\":\"))\n",
    "        return h * 3600 + m * 60 + s\n",
    "    return None\n",
    "\n",
    "# UDF 등록\n",
    "time_udf = udf(time_to_seconds, IntegerType())\n",
    "\n",
    "# 시간 컬럼 변환\n",
    "time_cols = [\"Swim Time\", \"Bike Time\", \"Run Time\", \"Overall Time\"]\n",
    "for col in time_cols:\n",
    "    df = df.withColumn(col, time_udf(df[col]))\n",
    "\n",
    "# 변환 결과 확인\n",
    "df.select(time_cols).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf405178",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Output column Gender_Index already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# StringIndexer를 적용하여 데이터 변환\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indexer \u001b[38;5;129;01min\u001b[39;00m indexers:\n\u001b[0;32m---> 12\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 변환된 데이터 확인\u001b[39;00m\n\u001b[1;32m     15\u001b[0m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGender_Index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDivision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDivision_Index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinish Status\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinish_Status_Index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Output column Gender_Index already exists."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# 문자열 컬럼을 인덱스로 변환\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"Gender\", outputCol=\"Gender_Index\"),\n",
    "    StringIndexer(inputCol=\"Division\", outputCol=\"Division_Index\"),\n",
    "    StringIndexer(inputCol=\"Finish Status\", outputCol=\"Finish_Status_Index\")\n",
    "]\n",
    "\n",
    "# StringIndexer를 적용하여 데이터 변환\n",
    "for indexer in indexers:\n",
    "    df = indexer.fit(df).transform(df)\n",
    "\n",
    "# 변환된 데이터 확인\n",
    "df.select(\"Gender\", \"Gender_Index\", \"Division\", \"Division_Index\", \"Finish Status\", \"Finish_Status_Index\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c15547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-------------------+\n",
      "|features                          |Finish_Status_Index|\n",
      "+----------------------------------+-------------------+\n",
      "|[202800.0,16330.0,11343.0,0.0,1.0]|0.0                |\n",
      "|[4500.0,15606.0,11479.0,0.0,2.0]  |0.0                |\n",
      "|[3620.0,16988.0,11843.0,0.0,6.0]  |0.0                |\n",
      "|[3953.0,17581.0,11888.0,0.0,3.0]  |0.0                |\n",
      "|[4100.0,16978.0,12360.0,0.0,12.0] |0.0                |\n",
      "|[3914.0,17221.0,12350.0,0.0,3.0]  |0.0                |\n",
      "|[202980.0,18517.0,11999.0,1.0,9.0]|0.0                |\n",
      "|[4035.0,17497.0,12528.0,0.0,3.0]  |0.0                |\n",
      "|[4685.0,16890.0,12786.0,0.0,1.0]  |0.0                |\n",
      "|[4216.0,17272.0,12884.0,0.0,1.0]  |0.0                |\n",
      "|[4568.0,18611.0,11773.0,0.0,3.0]  |0.0                |\n",
      "|[4226.0,18403.0,12561.0,0.0,1.0]  |0.0                |\n",
      "|[4540.0,17638.0,13072.0,0.0,3.0]  |0.0                |\n",
      "|[4879.0,18235.0,12675.0,0.0,3.0]  |0.0                |\n",
      "|[4250.0,18753.0,12972.0,1.0,15.0] |0.0                |\n",
      "|[4211.0,18860.0,12955.0,0.0,3.0]  |0.0                |\n",
      "|[4011.0,17565.0,14673.0,0.0,0.0]  |0.0                |\n",
      "|[191700.0,18746.0,14163.0,0.0,3.0]|0.0                |\n",
      "|[4861.0,19610.0,12134.0,0.0,2.0]  |0.0                |\n",
      "|[4450.0,19067.0,13515.0,0.0,0.0]  |0.0                |\n",
      "+----------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# feature 컬럼 정의\n",
    "feature_cols = [\"Swim Time\", \"Bike Time\", \"Run Time\", \"Gender_Index\", \"Division_Index\"]\n",
    "\n",
    "# VectorAssembler로 feature 벡터 생성\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# feature와 label 확인\n",
    "df.select(\"features\", \"Finish_Status_Index\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5537fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null 값 처리: 평균값으로 대체하거나 제거\n",
    "from pyspark.sql.functions import col, mean\n",
    "\n",
    "# 각 feature 컬럼의 평균값으로 null 대체\n",
    "for col_name in [\"Swim Time\", \"Bike Time\", \"Run Time\"]:\n",
    "    mean_value = df.select(mean(col(col_name))).collect()[0][0]\n",
    "    df = df.fillna({col_name: mean_value})\n",
    "\n",
    "# 다른 컬럼들도 null 값 확인 후 대체\n",
    "df = df.na.fill({\"Gender_Index\": 0.0, \"Division_Index\": 0.0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2ffabd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Swim Time\", \"Bike Time\", \"Run Time\", \"Gender_Index\", \"Division_Index\"],\n",
    "    outputCol=\"new_features\",  # 새로운 이름 지정\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd96db7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|features                          |\n",
      "+----------------------------------+\n",
      "|[202800.0,16330.0,11343.0,0.0,1.0]|\n",
      "|[4500.0,15606.0,11479.0,0.0,2.0]  |\n",
      "|[3620.0,16988.0,11843.0,0.0,6.0]  |\n",
      "|[3953.0,17581.0,11888.0,0.0,3.0]  |\n",
      "|[4100.0,16978.0,12360.0,0.0,12.0] |\n",
      "|[3914.0,17221.0,12350.0,0.0,3.0]  |\n",
      "|[202980.0,18517.0,11999.0,1.0,9.0]|\n",
      "|[4035.0,17497.0,12528.0,0.0,3.0]  |\n",
      "|[4685.0,16890.0,12786.0,0.0,1.0]  |\n",
      "|[4216.0,17272.0,12884.0,0.0,1.0]  |\n",
      "|[4568.0,18611.0,11773.0,0.0,3.0]  |\n",
      "|[4226.0,18403.0,12561.0,0.0,1.0]  |\n",
      "|[4540.0,17638.0,13072.0,0.0,3.0]  |\n",
      "|[4879.0,18235.0,12675.0,0.0,3.0]  |\n",
      "|[4250.0,18753.0,12972.0,1.0,15.0] |\n",
      "|[4211.0,18860.0,12955.0,0.0,3.0]  |\n",
      "|[4011.0,17565.0,14673.0,0.0,0.0]  |\n",
      "|[191700.0,18746.0,14163.0,0.0,3.0]|\n",
      "|[4861.0,19610.0,12134.0,0.0,2.0]  |\n",
      "|[4450.0,19067.0,13515.0,0.0,0.0]  |\n",
      "+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40af34c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 600\n",
      "Test dataset size: 115\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 분리 (80% 학습, 20% 테스트)\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(f\"Train dataset size: {train.count()}\")\n",
    "print(f\"Test dataset size: {test.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bd017f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-------------------+----------+-----------------------+\n",
      "|features                          |Finish_Status_Index|prediction|probability            |\n",
      "+----------------------------------+-------------------+----------+-----------------------+\n",
      "|[4250.0,18753.0,12972.0,1.0,15.0] |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[194640.0,20049.0,16236.0,0.0,4.0]|0.0                |0.0       |[0.98,0.02,0.0,0.0,0.0]|\n",
      "|[5879.0,18850.0,17612.0,0.0,14.0] |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[4433.0,20703.0,15461.0,0.0,3.0]  |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[4226.0,18403.0,12561.0,0.0,1.0]  |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[7276.0,22895.0,18066.0,0.0,0.0]  |1.0                |1.0       |[0.0,1.0,0.0,0.0,0.0]  |\n",
      "|[4872.0,19360.0,13485.0,0.0,4.0]  |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[4035.0,17497.0,12528.0,0.0,3.0]  |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[4011.0,17565.0,14673.0,0.0,0.0]  |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[5220.0,19356.0,15491.0,0.0,0.0]  |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[5046.0,18845.0,14308.0,0.0,0.0]  |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[4864.0,19765.0,14284.0,0.0,2.0]  |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[5530.0,23187.0,16637.0,1.0,10.0] |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[5552.0,20947.0,20734.0,0.0,4.0]  |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[7276.0,22895.0,18066.0,0.0,16.0] |1.0                |1.0       |[0.0,1.0,0.0,0.0,0.0]  |\n",
      "|[7276.0,22895.0,18066.0,1.0,11.0] |1.0                |1.0       |[0.0,1.0,0.0,0.0,0.0]  |\n",
      "|[7276.0,22895.0,18066.0,1.0,11.0] |1.0                |1.0       |[0.0,1.0,0.0,0.0,0.0]  |\n",
      "|[7276.0,22895.0,18066.0,1.0,9.0]  |1.0                |1.0       |[0.0,1.0,0.0,0.0,0.0]  |\n",
      "|[5736.0,26814.0,23675.0,1.0,9.0]  |0.0                |0.0       |[1.0,0.0,0.0,0.0,0.0]  |\n",
      "|[7276.0,22895.0,18066.0,0.0,6.0]  |1.0                |1.0       |[0.0,1.0,0.0,0.0,0.0]  |\n",
      "+----------------------------------+-------------------+----------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# 1. RandomForestClassifier 모델 설정\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    labelCol=\"Finish_Status_Index\",  # 타겟 값 (완주 여부)\n",
    "    featuresCol=\"features\",          # 학습에 사용될 피처\n",
    "    numTrees=100,                    # 트리 개수\n",
    "    maxDepth=5,                      # 트리의 최대 깊이\n",
    "    seed=42                          # 재현성을 위한 랜덤 시드\n",
    ")\n",
    "\n",
    "# 2. 모델 학습\n",
    "model = rf_classifier.fit(train)\n",
    "\n",
    "# 3. 테스트 데이터로 예측\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# 4. 예측 결과 확인\n",
    "predictions.select(\"features\", \"Finish_Status_Index\", \"prediction\", \"probability\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa198e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 다중 클래스 분류 평가기 사용\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Finish_Status_Index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d940cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [(5000.0, 18000.0, 13000.0, 0.0, 1.0)]\n",
    "columns = [\"Swim Time\", \"Bike Time\", \"Run Time\", \"Gender_Index\", \"Division_Index\"]\n",
    "new_df = spark.createDataFrame(new_data, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2499c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = assembler.transform(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6f8e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------+---------------------+\n",
      "|features                        |prediction|probability          |\n",
      "+--------------------------------+----------+---------------------+\n",
      "|[5000.0,18000.0,13000.0,0.0,1.0]|0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "+--------------------------------+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(new_features)\n",
    "prediction.select(\"features\", \"prediction\", \"probability\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea5e1bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----------+---------------------+\n",
      "|features                         |prediction|probability          |\n",
      "+---------------------------------+----------+---------------------+\n",
      "|[4000.0,18000.0,13000.0,0.0,5.0] |0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "|[3500.0,20000.0,15000.0,1.0,8.0] |0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "|[5000.0,22000.0,14000.0,0.0,3.0] |0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "|[4800.0,21000.0,16000.0,1.0,10.0]|0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "|[3900.0,19000.0,12000.0,0.0,7.0] |0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "|[4100.0,25000.0,17000.0,1.0,6.0] |0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "|[4300.0,23000.0,15000.0,0.0,4.0] |0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "|[4700.0,24000.0,16000.0,1.0,2.0] |0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "|[4200.0,20000.0,14000.0,0.0,9.0] |0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "|[3900.0,22000.0,15000.0,1.0,1.0] |0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "+---------------------------------+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# VectorAssembler를 사용해 features 열 추가\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Swim Time\", \"Bike Time\", \"Run Time\", \"Gender_Index\", \"Division_Index\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# features 열 생성\n",
    "test_df_with_features = assembler.transform(test_df)\n",
    "\n",
    "# 모델 예측\n",
    "test_predictions = model.transform(test_df_with_features)\n",
    "\n",
    "# 결과 확인\n",
    "test_predictions.select(\"features\", \"prediction\", \"probability\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cf5d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22257763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
