{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb8ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1669358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 16:56:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------+------+----+--------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "|bib|name                |country       |gender|div |div_rank|overall_time|overall_rank|swim_time|swim_rank|bike_time|bike_rank|run_time|run_rank|finish_status|\n",
      "+---+--------------------+--------------+------+----+--------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "|8  |Gustav Iden         |Norway        |Male  |MPRO|1       |7:40:24     |1           |48:23:00 |10       |4:11:06  |6        |2:36:15 |1       |Finisher     |\n",
      "|15 |Sam Laidlow         |France        |Male  |MPRO|2       |7:42:24     |2           |48:16:00 |2        |4:04:36  |1        |2:44:40 |5       |Finisher     |\n",
      "|1  |Kristian Blummenfelt|Norway        |Male  |MPRO|3       |7:43:23     |3           |48:20:00 |5        |4:11:16  |8        |2:39:21 |2       |Finisher     |\n",
      "|23 |Max Neumann         |Australia     |Male  |MPRO|4       |7:44:44     |4           |48:25:00 |13       |4:11:30  |9        |2:40:14 |3       |Finisher     |\n",
      "|17 |Joe Skipper         |United Kingdom|Male  |MPRO|5       |7:54:05     |5           |52:55:00 |60       |4:11:11  |7        |2:45:26 |6       |Finisher     |\n",
      "|7  |Sebastian Kienle    |Germany       |Male  |MPRO|6       |7:55:40     |6           |52:58:00 |66       |4:09:11  |4        |2:48:45 |13      |Finisher     |\n",
      "|12 |Leon Chevalier      |France        |Male  |MPRO|7       |7:55:52     |7           |52:54:00 |59       |4:09:05  |3        |2:49:28 |15      |Finisher     |\n",
      "|32 |Magnus Ditlev       |Denmark       |Male  |MPRO|8       |7:56:38     |8           |49:49:00 |32       |4:13:38  |11       |2:48:11 |11      |Finisher     |\n",
      "|38 |Clement Mignon      |France        |Male  |MPRO|9       |7:56:58     |9           |49:50:00 |33       |4:15:14  |14       |2:46:00 |8       |Finisher     |\n",
      "|6  |Patrick Lange       |Germany       |Male  |MPRO|10      |7:58:20     |10          |49:42:00 |26       |4:21:52  |22       |2:41:59 |4       |Finisher     |\n",
      "|11 |Cameron Wurf        |Australia     |Male  |MPRO|11      |8:00:51     |11          |52:51:00 |56       |4:09:04  |2        |2:54:27 |19      |Finisher     |\n",
      "|5  |Florian Angert      |Germany       |Male  |MPRO|12      |8:01:53     |12          |48:15:00 |1        |4:17:58  |19       |2:50:29 |16      |Finisher     |\n",
      "|9  |Timothy O'Donnell   |United States |Male  |MPRO|13      |8:02:58     |13          |48:23:00 |11       |4:13:30  |10       |2:56:03 |21      |Finisher     |\n",
      "|21 |Denis Chevrot       |France        |Male  |MPRO|14      |8:03:24     |14          |48:26:00 |16       |4:22:59  |25       |2:47:03 |9       |Finisher     |\n",
      "|20 |Matthew Hanson      |United States |Male  |MPRO|15      |8:04:55     |15          |52:40:00 |51       |4:22:18  |24       |2:45:34 |7       |Finisher     |\n",
      "|44 |Mathias Petersen    |Denmark       |Male  |MPRO|16      |8:06:45     |16          |48:25:00 |14       |4:24:55  |31       |2:48:16 |12      |Finisher     |\n",
      "|33 |Bradley Weiss       |South Africa  |Male  |MPRO|17      |8:07:28     |17          |49:41:00 |25       |4:24:49  |30       |2:48:01 |10      |Finisher     |\n",
      "|47 |Luciano Taccone     |Argentina     |Male  |MPRO|18      |8:09:10     |18          |49:47:00 |30       |4:25:08  |33       |2:49:18 |14      |Finisher     |\n",
      "|52 |Henrik Goesch       |Finland       |Male  |MPRO|19      |8:10:25     |19          |49:47:00 |31       |4:21:57  |23       |2:53:49 |18      |Finisher     |\n",
      "|19 |Rudy Von Berg       |United States |Male  |MPRO|20      |8:12:47     |20          |49:43:00 |28       |4:15:24  |15       |3:02:17 |34      |Finisher     |\n",
      "+---+--------------------+--------------+------+----+--------+------------+------------+---------+---------+---------+---------+--------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- bib: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- div: string (nullable = true)\n",
      " |-- div_rank: integer (nullable = true)\n",
      " |-- overall_time: string (nullable = true)\n",
      " |-- overall_rank: integer (nullable = true)\n",
      " |-- swim_time: string (nullable = true)\n",
      " |-- swim_rank: integer (nullable = true)\n",
      " |-- bike_time: string (nullable = true)\n",
      " |-- bike_rank: integer (nullable = true)\n",
      " |-- run_time: string (nullable = true)\n",
      " |-- run_rank: integer (nullable = true)\n",
      " |-- finish_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark 세션 초기화\n",
    "spark = SparkSession.builder.appName(\"Ironman Data Analysis_04\").getOrCreate()\n",
    "\n",
    "# CSV 다시 불러오기\n",
    "file_path = \"file:///home/lab12/src/data/ironman_wc_2022.csv\"  # 정확한 경로 입력\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# 데이터 확인\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2815a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+---------------+---+\n",
      "|swim_seconds|bike_seconds|run_seconds|overall_seconds|DNF|\n",
      "+------------+------------+-----------+---------------+---+\n",
      "|      174180|       15066|       9375|          27624|  1|\n",
      "|      173760|       14676|       9880|          27744|  1|\n",
      "|      174000|       15076|       9561|          27803|  1|\n",
      "|      174300|       15090|       9614|          27884|  1|\n",
      "|      190500|       15071|       9926|          28445|  1|\n",
      "|      190680|       14951|      10125|          28540|  1|\n",
      "|      190440|       14945|      10168|          28552|  1|\n",
      "|      179340|       15218|      10091|          28598|  1|\n",
      "|      179400|       15314|       9960|          28618|  1|\n",
      "|      178920|       15712|       9719|          28700|  1|\n",
      "|      190260|       14944|      10467|          28851|  1|\n",
      "|      173700|       15478|      10229|          28913|  1|\n",
      "|      174180|       15210|      10563|          28978|  1|\n",
      "|      174360|       15779|      10023|          29004|  1|\n",
      "|      189600|       15738|       9934|          29095|  1|\n",
      "|      174300|       15895|      10096|          29205|  1|\n",
      "|      178860|       15889|      10081|          29248|  1|\n",
      "|      179220|       15908|      10158|          29350|  1|\n",
      "|      179220|       15717|      10429|          29425|  1|\n",
      "|      178980|       15324|      10937|          29567|  1|\n",
      "+------------+------------+-----------+---------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col, split\n",
    "\n",
    "# 1. 시간 데이터를 초 단위로 변환하는 함수 정의\n",
    "def time_to_seconds(time_str_col):\n",
    "    \"\"\"\n",
    "    문자열 형태의 시간 데이터를 초 단위로 변환\n",
    "    \"\"\"\n",
    "    return (split(time_str_col, \":\")[0].cast(\"int\") * 3600 +\n",
    "            split(time_str_col, \":\")[1].cast(\"int\") * 60 +\n",
    "            split(time_str_col, \":\")[2].cast(\"int\"))\n",
    "\n",
    "# 2. 초 단위 컬럼 생성\n",
    "df = df.withColumn(\"swim_seconds\", time_to_seconds(col(\"swim_time\"))) \\\n",
    "       .withColumn(\"bike_seconds\", time_to_seconds(col(\"bike_time\"))) \\\n",
    "       .withColumn(\"run_seconds\", time_to_seconds(col(\"run_time\"))) \\\n",
    "       .withColumn(\"overall_seconds\", time_to_seconds(col(\"overall_time\")))\n",
    "\n",
    "# 3. 컷오프 기준에 따른 DNF 컬럼 생성\n",
    "df = df.withColumn(\n",
    "    \"DNF\",\n",
    "    when((col(\"swim_seconds\") > 2 * 3600 + 20 * 60) |  # 수영 컷오프\n",
    "         (col(\"swim_seconds\") + col(\"bike_seconds\") > 10 * 3600 + 30 * 60) |  # 수영 + 사이클 컷오프\n",
    "         (col(\"overall_seconds\") > 17 * 3600), 1).otherwise(0)  # 전체 컷오프\n",
    ")\n",
    "\n",
    "# 4. 결과 확인\n",
    "df.select(\"swim_seconds\", \"bike_seconds\", \"run_seconds\", \"overall_seconds\", \"DNF\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddbdd956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|finish_status_encoded|count|\n",
      "+---------------------+-----+\n",
      "|                    1| 2376|\n",
      "|                    0|   70|\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. DQ 데이터 제거\n",
    "df = df.filter(col(\"finish_status\") != \"DQ\")\n",
    "\n",
    "# 2. 완주 여부 컬럼 생성 (DNF=0, Finisher=1)\n",
    "df = df.withColumn(\n",
    "    \"finish_status_encoded\",\n",
    "    when(col(\"finish_status\") == \"DNF\", 0).otherwise(1)\n",
    ")\n",
    "\n",
    "# 3. 결과 확인\n",
    "df.groupBy(\"finish_status_encoded\").count().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c228c7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|div   |age_group|\n",
      "+------+---------+\n",
      "|M30-34|30       |\n",
      "|M25-29|25       |\n",
      "|M18-24|18       |\n",
      "|M35-39|35       |\n",
      "|M40-44|40       |\n",
      "|M55-59|55       |\n",
      "|M45-49|45       |\n",
      "|MPRO  |0        |\n",
      "|M50-54|50       |\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# 1. div 컬럼에서 나이대 추출\n",
    "df = df.withColumn(\"age_group\", regexp_extract(col(\"div\"), r\"(\\d+)-\", 1).cast(\"int\"))\n",
    "\n",
    "# 2. null 값 처리 (age_group이 없는 경우 기본값 0으로 대체)\n",
    "df = df.fillna({\"age_group\": 0})\n",
    "\n",
    "# 3. 결과 확인\n",
    "df.select(\"div\", \"age_group\").distinct().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bad631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|   div|age_group|\n",
      "+------+---------+\n",
      "|M30-34|       30|\n",
      "|M25-29|       25|\n",
      "|M18-24|       18|\n",
      "|M35-39|       35|\n",
      "|M40-44|       40|\n",
      "|M55-59|       55|\n",
      "|M45-49|       45|\n",
      "|  MPRO|        0|\n",
      "|M50-54|       50|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MPRO 처리 없이 나머지 데이터 유지\n",
    "df.select(\"div\", \"age_group\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e35e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------------------+\n",
      "|features                                  |finish_status_encoded|\n",
      "+------------------------------------------+---------------------+\n",
      "|[1.0,0.0,174180.0,15066.0,9375.0,27624.0] |1                    |\n",
      "|[1.0,0.0,173760.0,14676.0,9880.0,27744.0] |1                    |\n",
      "|[1.0,0.0,174000.0,15076.0,9561.0,27803.0] |1                    |\n",
      "|[1.0,0.0,174300.0,15090.0,9614.0,27884.0] |1                    |\n",
      "|[1.0,0.0,190500.0,15071.0,9926.0,28445.0] |1                    |\n",
      "|[1.0,0.0,190680.0,14951.0,10125.0,28540.0]|1                    |\n",
      "|[1.0,0.0,190440.0,14945.0,10168.0,28552.0]|1                    |\n",
      "|[1.0,0.0,179340.0,15218.0,10091.0,28598.0]|1                    |\n",
      "|[1.0,0.0,179400.0,15314.0,9960.0,28618.0] |1                    |\n",
      "|[1.0,0.0,178920.0,15712.0,9719.0,28700.0] |1                    |\n",
      "|[1.0,0.0,190260.0,14944.0,10467.0,28851.0]|1                    |\n",
      "|[1.0,0.0,173700.0,15478.0,10229.0,28913.0]|1                    |\n",
      "|[1.0,0.0,174180.0,15210.0,10563.0,28978.0]|1                    |\n",
      "|[1.0,0.0,174360.0,15779.0,10023.0,29004.0]|1                    |\n",
      "|[1.0,0.0,189600.0,15738.0,9934.0,29095.0] |1                    |\n",
      "|[1.0,0.0,174300.0,15895.0,10096.0,29205.0]|1                    |\n",
      "|[1.0,0.0,178860.0,15889.0,10081.0,29248.0]|1                    |\n",
      "|[1.0,0.0,179220.0,15908.0,10158.0,29350.0]|1                    |\n",
      "|[1.0,0.0,179220.0,15717.0,10429.0,29425.0]|1                    |\n",
      "|[1.0,0.0,178980.0,15324.0,10937.0,29567.0]|1                    |\n",
      "+------------------------------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Feature 벡터 생성\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\", \"overall_seconds\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_final = assembler.transform(df).select(\"features\", \"finish_status_encoded\")\n",
    "\n",
    "# 결과 확인\n",
    "df_final.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19ea9185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-----------------+-----------------+----------------+--------------------+-----+\n",
      "|gender_encoded_null|age_group_null|swim_seconds_null|bike_seconds_null|run_seconds_null|overall_seconds_null|count|\n",
      "+-------------------+--------------+-----------------+-----------------+----------------+--------------------+-----+\n",
      "|              false|         false|            false|            false|           false|               false| 2376|\n",
      "|              false|         false|             true|             true|            true|                true|    4|\n",
      "|              false|         false|            false|             true|            true|                true|   38|\n",
      "|              false|         false|            false|            false|            true|                true|   28|\n",
      "+-------------------+--------------+-----------------+-----------------+----------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col(\"gender_encoded\").isNull().alias(\"gender_encoded_null\"),\n",
    "    col(\"age_group\").isNull().alias(\"age_group_null\"),\n",
    "    col(\"swim_seconds\").isNull().alias(\"swim_seconds_null\"),\n",
    "    col(\"bike_seconds\").isNull().alias(\"bike_seconds_null\"),\n",
    "    col(\"run_seconds\").isNull().alias(\"run_seconds_null\"),\n",
    "    col(\"overall_seconds\").isNull().alias(\"overall_seconds_null\")\n",
    ").groupBy(\n",
    "    \"gender_encoded_null\", \"age_group_null\", \"swim_seconds_null\", \n",
    "    \"bike_seconds_null\", \"run_seconds_null\", \"overall_seconds_null\"\n",
    ").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ce4b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+----------------+--------------------+-----+\n",
      "|swim_seconds_null|bike_seconds_null|run_seconds_null|overall_seconds_null|count|\n",
      "+-----------------+-----------------+----------------+--------------------+-----+\n",
      "|            false|            false|           false|               false| 2442|\n",
      "+-----------------+-----------------+----------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# 1. 모든 시간이 null인 행 제거\n",
    "df = df.filter(~(\n",
    "    col(\"swim_seconds\").isNull() & \n",
    "    col(\"bike_seconds\").isNull() & \n",
    "    col(\"run_seconds\").isNull() & \n",
    "    col(\"overall_seconds\").isNull()\n",
    "))\n",
    "\n",
    "# 2. 나머지 null 값을 평균값으로 대체\n",
    "avg_values = df.select(\n",
    "    mean(\"swim_seconds\").alias(\"avg_swim\"),\n",
    "    mean(\"bike_seconds\").alias(\"avg_bike\"),\n",
    "    mean(\"run_seconds\").alias(\"avg_run\"),\n",
    "    mean(\"overall_seconds\").alias(\"avg_overall\")\n",
    ").first()\n",
    "\n",
    "df = df.fillna({\n",
    "    \"swim_seconds\": avg_values[\"avg_swim\"],\n",
    "    \"bike_seconds\": avg_values[\"avg_bike\"],\n",
    "    \"run_seconds\": avg_values[\"avg_run\"],\n",
    "    \"overall_seconds\": avg_values[\"avg_overall\"]\n",
    "})\n",
    "\n",
    "# 처리 결과 확인\n",
    "df.select(\n",
    "    col(\"swim_seconds\").isNull().alias(\"swim_seconds_null\"),\n",
    "    col(\"bike_seconds\").isNull().alias(\"bike_seconds_null\"),\n",
    "    col(\"run_seconds\").isNull().alias(\"run_seconds_null\"),\n",
    "    col(\"overall_seconds\").isNull().alias(\"overall_seconds_null\")\n",
    ").groupBy(\n",
    "    \"swim_seconds_null\", \"bike_seconds_null\", \"run_seconds_null\", \"overall_seconds_null\"\n",
    ").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5b79585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------------------+\n",
      "|features                                  |finish_status_encoded|\n",
      "+------------------------------------------+---------------------+\n",
      "|[1.0,0.0,174180.0,15066.0,9375.0,27624.0] |1                    |\n",
      "|[1.0,0.0,173760.0,14676.0,9880.0,27744.0] |1                    |\n",
      "|[1.0,0.0,174000.0,15076.0,9561.0,27803.0] |1                    |\n",
      "|[1.0,0.0,174300.0,15090.0,9614.0,27884.0] |1                    |\n",
      "|[1.0,0.0,190500.0,15071.0,9926.0,28445.0] |1                    |\n",
      "|[1.0,0.0,190680.0,14951.0,10125.0,28540.0]|1                    |\n",
      "|[1.0,0.0,190440.0,14945.0,10168.0,28552.0]|1                    |\n",
      "|[1.0,0.0,179340.0,15218.0,10091.0,28598.0]|1                    |\n",
      "|[1.0,0.0,179400.0,15314.0,9960.0,28618.0] |1                    |\n",
      "|[1.0,0.0,178920.0,15712.0,9719.0,28700.0] |1                    |\n",
      "|[1.0,0.0,190260.0,14944.0,10467.0,28851.0]|1                    |\n",
      "|[1.0,0.0,173700.0,15478.0,10229.0,28913.0]|1                    |\n",
      "|[1.0,0.0,174180.0,15210.0,10563.0,28978.0]|1                    |\n",
      "|[1.0,0.0,174360.0,15779.0,10023.0,29004.0]|1                    |\n",
      "|[1.0,0.0,189600.0,15738.0,9934.0,29095.0] |1                    |\n",
      "|[1.0,0.0,174300.0,15895.0,10096.0,29205.0]|1                    |\n",
      "|[1.0,0.0,178860.0,15889.0,10081.0,29248.0]|1                    |\n",
      "|[1.0,0.0,179220.0,15908.0,10158.0,29350.0]|1                    |\n",
      "|[1.0,0.0,179220.0,15717.0,10429.0,29425.0]|1                    |\n",
      "|[1.0,0.0,178980.0,15324.0,10937.0,29567.0]|1                    |\n",
      "+------------------------------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Feature 벡터 생성\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"gender_encoded\", \"age_group\", \"swim_seconds\", \"bike_seconds\", \"run_seconds\", \"overall_seconds\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_final = assembler.transform(df).select(\"features\", \"finish_status_encoded\")\n",
    "\n",
    "# 결과 확인\n",
    "df_final.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9f42453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 크기: 2004\n",
      "테스트 데이터 크기: 438\n"
     ]
    }
   ],
   "source": [
    "# 훈련 및 테스트 데이터 분리\n",
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(\"훈련 데이터 크기:\", train_data.count())\n",
    "print(\"테스트 데이터 크기:\", test_data.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51522f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 17:09:18 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/12/18 17:09:18 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.48\n",
      "+-------------------------------------------+---------------------+----------+-----------------------------------------+\n",
      "|features                                   |finish_status_encoded|prediction|probability                              |\n",
      "+-------------------------------------------+---------------------+----------+-----------------------------------------+\n",
      "|[1.0,0.0,173880.0,15294.0,11157.0,29621.0] |1                    |1.0       |[0.03598355283431645,0.9640164471656836] |\n",
      "|[1.0,0.0,174000.0,15806.0,10809.0,29796.0] |1                    |1.0       |[0.03902208296993068,0.9609779170300693] |\n",
      "|[1.0,0.0,174180.0,15066.0,9375.0,27624.0]  |1                    |1.0       |[0.03552577338897682,0.9644742266110232] |\n",
      "|[1.0,0.0,174300.0,19537.0,15172.0,39636.0] |0                    |1.0       |[0.04032886603580138,0.9596711339641986] |\n",
      "|[1.0,0.0,178440.0,15955.0,15172.0,39636.0] |0                    |1.0       |[0.00850189787448286,0.9914981021255171] |\n",
      "|[1.0,0.0,178920.0,16716.0,12209.0,32262.0] |1                    |1.0       |[0.04255250318198374,0.9574474968180162] |\n",
      "|[1.0,0.0,179400.0,15314.0,9960.0,28618.0]  |1                    |1.0       |[0.03537233184863594,0.964627668151364]  |\n",
      "|[1.0,0.0,186300.0,15958.0,10972.0,30335.0] |1                    |1.0       |[0.03837426576328015,0.9616257342367198] |\n",
      "|[1.0,0.0,190680.0,14951.0,10125.0,28540.0] |1                    |1.0       |[0.03298912420565543,0.9670108757943445] |\n",
      "|[1.0,0.0,190680.0,15391.0,11873.0,30748.0] |1                    |1.0       |[0.034252661258841556,0.9657473387411585]|\n",
      "|[1.0,0.0,201900.0,19537.0,15172.0,39636.0] |0                    |1.0       |[0.04252966911610869,0.9574703308838913] |\n",
      "|[1.0,0.0,211800.0,16080.0,10990.0,30896.0] |1                    |1.0       |[0.03657503026386801,0.963424969736132]  |\n",
      "|[1.0,18.0,3693.0,19738.0,17137.0,41395.0]  |1                    |1.0       |[0.027973600237414697,0.9720263997625853]|\n",
      "|[1.0,18.0,3732.0,18024.0,16481.0,38843.0]  |1                    |1.0       |[0.022737129858447194,0.9772628701415528]|\n",
      "|[1.0,18.0,3920.0,17882.0,15172.0,39636.0]  |0                    |1.0       |[0.012064284471156086,0.987935715528844] |\n",
      "|[1.0,18.0,4084.0,18252.0,15730.0,38554.0]  |1                    |1.0       |[0.022327739231933853,0.9776722607680661]|\n",
      "|[1.0,18.0,4228.0,19186.0,15846.0,39906.0]  |1                    |1.0       |[0.02371690300343344,0.9762830969965666] |\n",
      "|[1.0,18.0,193620.0,16888.0,17942.0,38571.0]|1                    |1.0       |[0.031916415387750884,0.9680835846122491]|\n",
      "|[1.0,18.0,209280.0,17080.0,12121.0,33207.0]|1                    |1.0       |[0.03423344856892694,0.965766551431073]  |\n",
      "|[1.0,18.0,213780.0,22759.0,20056.0,47353.0]|1                    |1.0       |[0.06352041562925116,0.9364795843707489] |\n",
      "+-------------------------------------------+---------------------+----------+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Logistic Regression 모델 생성 및 학습\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"finish_status_encoded\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# 테스트 데이터로 예측\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# ROC-AUC 평가\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"finish_status_encoded\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}\")\n",
    "predictions.select(\"features\", \"finish_status_encoded\", \"prediction\", \"probability\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76bb4b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------+------------------------------------------+\n",
      "|features                                  |prediction|probability                               |\n",
      "+------------------------------------------+----------+------------------------------------------+\n",
      "|[1.0,30.0,8000.0,15000.0,10000.0,33000.0] |1.0       |[0.005010171809519066,0.994989828190481]  |\n",
      "|[0.0,25.0,8500.0,15500.0,11000.0,35000.0] |1.0       |[0.004845942754198488,0.9951540572458015] |\n",
      "|[1.0,40.0,9000.0,16000.0,11500.0,36500.0] |1.0       |[0.003975598217536807,0.9960244017824632] |\n",
      "|[0.0,35.0,9000.0,18000.0,14000.0,41000.0] |1.0       |[0.00550199619540738,0.9944980038045926]  |\n",
      "|[1.0,50.0,12000.0,20000.0,15000.0,47000.0]|1.0       |[0.002806643511658182,0.9971933564883418] |\n",
      "|[1.0,20.0,9000.0,16000.0,11500.0,36500.0] |1.0       |[0.00472168102266512,0.9952783189773349]  |\n",
      "|[0.0,28.0,7200.0,14000.0,9500.0,30700.0]  |1.0       |[0.005488540517465433,0.9945114594825346] |\n",
      "|[1.0,32.0,8500.0,18000.0,14500.0,41000.0] |1.0       |[0.006453538009090801,0.9935464619909092] |\n",
      "|[0.0,45.0,9000.0,20000.0,15000.0,47000.0] |1.0       |[0.0029125900843616354,0.9970874099156384]|\n",
      "|[1.0,18.0,7500.0,13000.0,9000.0,29500.0]  |1.0       |[0.004720433979714138,0.9952795660202859] |\n",
      "+------------------------------------------+----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# 1. 임의 데이터 생성 완주가 불가능할 데이터도 포함해서 넣어보기로함\n",
    "# 각 데이터는 [gender_encoded, age_group, swim_seconds, bike_seconds, run_seconds, overall_seconds]\n",
    "test_data_manual = [\n",
    "    Row(features=Vectors.dense([1.0, 30.0, 8000.0, 15000.0, 10000.0, 33000.0])),  # 완주 가능\n",
    "    Row(features=Vectors.dense([0.0, 25.0, 8500.0, 15500.0, 11000.0, 35000.0])),  # 완주 가능\n",
    "    Row(features=Vectors.dense([1.0, 40.0, 9000.0, 16000.0, 11500.0, 36500.0])),  # 완주 가능\n",
    "    Row(features=Vectors.dense([0.0, 35.0, 9000.0, 18000.0, 14000.0, 41000.0])),  # 완주 불가능 (전체 초과)\n",
    "    Row(features=Vectors.dense([1.0, 50.0, 12000.0, 20000.0, 15000.0, 47000.0])),  # 완주 불가능 (전체 초과)\n",
    "    Row(features=Vectors.dense([1.0, 20.0, 9000.0, 16000.0, 11500.0, 36500.0])),  # 완주 가능\n",
    "    Row(features=Vectors.dense([0.0, 28.0, 7200.0, 14000.0, 9500.0, 30700.0])),   # 완주 가능\n",
    "    Row(features=Vectors.dense([1.0, 32.0, 8500.0, 18000.0, 14500.0, 41000.0])),  # 완주 불가능 (전체 초과)\n",
    "    Row(features=Vectors.dense([0.0, 45.0, 9000.0, 20000.0, 15000.0, 47000.0])),  # 완주 불가능 (전체 초과)\n",
    "    Row(features=Vectors.dense([1.0, 18.0, 7500.0, 13000.0, 9000.0, 29500.0])),   # 완주 가능\n",
    "]\n",
    "\n",
    "# 2. 임의 데이터를 DataFrame으로 변환\n",
    "test_df_manual = spark.createDataFrame(test_data_manual)\n",
    "\n",
    "# 3. 모델로 예측\n",
    "predictions_manual = lr_model.transform(test_df_manual)\n",
    "\n",
    "# 4. 결과 출력\n",
    "predictions_manual.select(\"features\", \"prediction\", \"probability\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674eb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 말도안되는 기록으로 넣어도 완주를 예측하는 모델이 나옴... 처음부터 다시 꽦..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c202e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b60138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
